{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA 2018 - Homework 3\n",
    "\n",
    "\n",
    "\n",
    "## Undestanding the StackOverflow community\n",
    "\n",
    "\n",
    "Deadline: Nov 7th 2018, 23:59:59\n",
    "\n",
    "Submission link: Check channel homework-3-public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import country_converter as coco\n",
    "cc = coco.CountryConverter()\n",
    "from branca.colormap import linear\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ada_const'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f46d467e447b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mada_const\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mada_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#from ada_imports import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ada_const'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import folium\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import unix_timestamp, udf, to_date, to_timestamp\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from ada_const import *\n",
    "from ada_context import *\n",
    "from ada_imports import *\n",
    "from ada_dataIO import *\n",
    "from ada_dataCleaner import *\n",
    "\n",
    "import json\n",
    "import folium\n",
    "from folium.plugins import HeatMap, HeatMapWithTime, TimeSliderChoropleth\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='ADAforever1', api_key='TTknVnm05Dc3wX5nddyo')\n",
    "\n",
    "OUT_DIR = 'output'\n",
    "DATA_DIR = './data_sample'\n",
    "\n",
    "\n",
    "def save(df):\n",
    "    df.write.mode('overwrite').csv('df_save')\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "sc = spark.sparkContext\n",
    "\n",
    "events = spark.read.csv(os.path.join(DATA_DIR, \"*.export.CSV\"), sep=\"\\t\", schema=EVENTS_SCHEMA)\n",
    "mentions = spark.read.csv(os.path.join(DATA_DIR, \"*.mentions.CSV\"), sep=\"\\t\", schema=MENTIONS_SCHEMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanEvents(events_df):\n",
    "    if events_df is None:\n",
    "        return None\n",
    "    global isoCodes\n",
    "    isoCodes = functions.udf(fips2iso, types.StringType())\n",
    "    tmp = events_df.select('GLOBALEVENTID',\n",
    "                           to_date(events_df.Day_DATE.cast('String'), 'yyyyMMdd').alias('date'),\n",
    "                           'MonthYear_Date',\n",
    "                           'Year_Date',\n",
    "                           'FractionDate',\n",
    "                           'EventCode',\n",
    "                           'EventRootCode',\n",
    "                           'QuadClass',\n",
    "                           round(events_df.GoldsteinScale, 2).alias('GoldsteinScale'),\n",
    "                           'AvgTone',\n",
    "                           'ActionGeo_CountryCode')\n",
    "\n",
    "    first_record = datetime.strptime('20150218', '%Y%m%d')\n",
    "    last_considered_events = datetime.strptime('20170921', '%Y%m%d')\n",
    "\n",
    "    tmp = tmp.filter(tmp['date'] >= first_record)\n",
    "    tmp = tmp.filter(tmp['date'] <= last_considered_events)\n",
    "    return tmp.select('GLOBALEVENTID',\n",
    "                      'date',\n",
    "                      dayofmonth(tmp.date).alias('Day_Date'),\n",
    "                      month(tmp.date).alias('Month_Date'),\n",
    "                      'Year_Date',\n",
    "                      'MonthYear_Date',\n",
    "                      'FractionDate',\n",
    "                      'EventCode',\n",
    "                      'EventRootCode',\n",
    "                      'QuadClass',\n",
    "                      'GoldsteinScale',\n",
    "                      'AvgTone',\n",
    "                      'ActionGeo_CountryCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = cleanEvents(events)\n",
    "mentions = cleanMentions(mentions)\n",
    "\n",
    "events.registerTempTable('events')\n",
    "mentions.registerTempTable('mentions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "For the exploration of the database, we will be using a sample of the data (mentions and events) provided on the cluster. The chosen samples were spread over time in order to have a more reliable estimation of the distribution of the data across time. Indeed, we took a file of mentions and events for each month from the beginning to the end of the recording of mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} events are reported in our events sample dataset'.format(events.count()))\n",
    "print('{} mentions are reported in our mentions sample dataset'.format(mentions.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that we have more mentions than we have events. This is consistent with the nature of the 2 databases. Indeed, the mentions database records for each event all the news sources in which the events were mentioned. Thus, an event is represented by as many mentions as there are news sources which mentioned it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When were the mentions collected ? \n",
    "def get_period_mentions(df_mentions):\n",
    "    \n",
    "    start = df_mentions.select('MentionTimeDate').orderBy('MentionTimeDate').head()\n",
    "    stop = df_mentions.select('MentionTimeDate').orderBy(desc('MentionTimeDate')).head()\n",
    "    \n",
    "    return start[0], stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop = get_period_mentions(mentions)\n",
    "print('Mentions collection started on {} and stoped on {}'.format(start,stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When did the events recorded in the sample of mentions take place? \n",
    "def get_period_events_mentions(df_mentions):\n",
    "    \n",
    "    start = df_mentions.select('EventTimeDate').orderBy('EventTimeDate').head()\n",
    "    stop = df_mentions.select('EventTimeDate').orderBy(desc('EventTimeDate')).head()\n",
    "    \n",
    "    return start[0], stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop = get_period_events_mentions(mentions)\n",
    "print('Events mentioned in the sample of mentions took place from {} to {}'.format(start,stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When did the recorded events take place?\n",
    "def get_period_events(df_events):\n",
    "    \n",
    "    start = df_events.select('date').orderBy('date').head()\n",
    "    stop = df_events.select('date').orderBy(desc('date')).head()\n",
    "    \n",
    "    return start[0], stop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop = get_period_events(events)\n",
    "print('Events recorded in the sample of events started on {} and stoped on {}'.format(start,stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first recorded events took place before the collection of mentions started. Indeed, mentions refering to events which happened in the past will lead to the recording of these events in the database. \n",
    "\n",
    "# Origin of our data\n",
    "\n",
    "The news reports we will be using accross this project are gathered from different types of news source : \n",
    "\n",
    "- Web sources (1)\n",
    "- Broadcasts, prints, other offline sources (2)\n",
    "- CORE archives (3)\n",
    "- DTIC archives (4)\n",
    "- JSTOR archives (5)\n",
    "- NonTextual sources (6)\n",
    "\n",
    "Let's explore the origin of the news reports we will be analysing :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val = ['Web Sources','Offline Sources','CORE Archives','DTIC Archives','JSTOR Archives','Non Textual Sources']\n",
    "\n",
    "def get_labels(labels):   \n",
    "    return [x for i,x in enumerate(labels_val) if (i+1) in labels]\n",
    "    \n",
    "def get_sources(df_mentions):\n",
    "    \n",
    "    sources = df_mentions.groupby('MentionType').agg(count('GLOBALEVENTID').alias('Number Mentions')).orderBy('MentionType')\n",
    "    \n",
    "    mentions_list = list(sources.select('Number Mentions').toPandas()['Number Mentions'])\n",
    "    source_list = list(sources.select('MentionType').toPandas()['MentionType'])\n",
    "\n",
    "    return pd.DataFrame({'Number Mentions': mentions_list},index= source_list)\n",
    "\n",
    "sources = get_sources(mentions)\n",
    "sources.plot.pie(y='Number Mentions', autopct='%.2f',figsize=(5, 5),labels=get_labels(list(sources.index)), title ='Origin of the news')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, almost all sources are websources. This is consistent with the habits of people today, who will tend to look at the news on their computer or on their phone while they are on their way. This lead to the developement of more websources to satisfy the readers. Not so many people are reading newspapers anymore. \n",
    "\n",
    "Now we would like to have a more specific overview of the different medias which report the news. We thus look at the medias which mentioned the most events over the 2 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions.select('MentionSourceName').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sources_names(df_mentions) :\n",
    "    return df_mentions.groupBy('MentionSourceName').count().orderBy(desc('count')).limit(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = get_sources_names(mentions.select('MentionSourceName'))\n",
    "sources.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that among the most prevalent medias are many american news sources like the broadcast and internet radio iheart.com (USA, Canada, New Zealand, Australia), the search engine yahoo.com, the newspaper/radio/television news agency ap.org, the american broadcasting company abc7 and other american news relating medias (news-sentinel, wnd, news12, tucson, miamiherald, sfgate, missoulian, washingtonpost, huffingtonpost...).\n",
    "\n",
    "The most prevalent medias also count the british international news agency reuters.com and newspaper DailyMail.co.uk, the Palestine news agency Wafa, the australian Courier Mail, the indian indiatimes and the chinese news agency xinhuanet.\n",
    "\n",
    "# Confidence in our data\n",
    "\n",
    "After seeing the sources of the mentions, we want to have an overview of the mentions themselfes. The mentions dataframe provides an estimation of the confidence at which an event was extracted from a news report. If a news report is focuses on an event, the confidence ratio at which this event is detected is higher than in news report where the event is only briefly mentionned.\n",
    "\n",
    "As a first insight in our data, let's see the distribution of this ratio accross our mention dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the percentage of mentions for each confidence value\n",
    "def get_confidence(df_mentions):\n",
    "    \n",
    "    total_mentions = df_mentions.count()\n",
    "    udf = UserDefinedFunction(lambda x: x/total_mentions, DoubleType())\n",
    "    \n",
    "    confidence = df_mentions.groupby('Confidence').agg(count('GLOBALEVENTID').alias('Number Mentions')).orderBy('Confidence')\n",
    "    confidence = confidence.select([udf(column).alias('Percentage of Mentions') \n",
    "                                    if column == 'Number Mentions' else column for column in confidence.columns])\n",
    "    \n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = get_confidence(mentions).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence.plot.bar(x='Confidence', y='Percentage of Mentions', figsize=(6, 6), colormap='Paired',\n",
    "                    title = 'Distribution of the mentions depending on their confidence ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, a consequent part of the reported mentions have a low confidence ratio. Indeed, more than 50% of the mentions have less than 50% confidence. This means that most of the recorded events are mentioned briefly in the news and have not entire news reports dedicated to them. \n",
    "\n",
    "We decided to investigate whether this repartition was different depending on the mention source type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,label in enumerate(labels_val):\n",
    "\n",
    "    sources_index = mentions['MentionType'] == str(index+1)\n",
    "    sources = mentions[['GLOBALEVENTID','MentionType', 'Confidence']][sources_index]\n",
    "    \n",
    "    if not sources.toPandas().empty :\n",
    "        confidence = get_confidence(sources).toPandas()\n",
    "        confidence.plot.bar(x='Confidence', y='Percentage of Mentions', figsize=(6, 6), \n",
    "                            colormap='Paired',title = 'Mentions Confidence distribution for '+label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous plots show that roughly 35 % of mentions in web sources have a confidence level of 20% or less. This lack of confidence is slightly stronger for the offline sources, which represent less than 1% of mentions sources. \n",
    "To get more reliable results through out our projets, the decision was made to consider only news mentions with a confidence level of at least 20%. Indeed, we want to study events which were seriously reported by the news and not briefly written down. We thus removed the mentions with a confidence inferior to 20% :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goodConfidence(df_mentions):\n",
    "   \n",
    "    index = df_mentions['Confidence'] > 20\n",
    "\n",
    "    return df_mentions[df_mentions.schema.names][index]\n",
    "\n",
    "mentions = get_goodConfidence(mentions)\n",
    "mentions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After removal of the mentions with less than 20% of condidence, {} mentions were left in our mentions sample dataset'.format(mentions.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentions , Mediatic Coverge and Mediatic Attention\n",
    "\n",
    "Our project will be centered around the evaluation of the bias of medias towards specific locations and event types worldwide. \n",
    "To achieve this, an indicator of media coverage and media attention should be defined. This will be achieved by using the mentions database, which reports every 15 minutes all news reports of events in the news. Each row in this database records a acticle/news report which mentioned at least once a specific event. \n",
    "\n",
    "#### Media Coverage :\n",
    "Media coverage will be defined as the number of mentions in news report that an event will receive within 120 days (4 months) posterior to its occurence. This time period was set to avoid the bias of the date of occurence of the event. Indeed, our database records mentions from February 2015 to November 2017. Each event should be considered on an equivalent period of time to have equal chance of being mentioned. Otherwise, events recorded at the beginning of the database will have more chance to be mentioned than events happening at the end of the recording.  \n",
    "As explicited above, another restriction will be applied to our mentions database : only mentions with a confidence ratio above 20% will be considered.\n",
    "\n",
    "#### Mediatic Attention:\n",
    "In order to grasp the level of attention perceived by an event on the mediactic scene, we should compute the mediatic attention index.  This index will be considered as the ratio between media coverage and the total amount of mentions reported in the database during the equivalent period of time. This normalisation procedure allows to avoid a biased perception and compare the level of atention perceived by events. \n",
    "\n",
    "Let's first observe the distribution of mediatic coverage by event within our database, after restricting the mentions to the ones recorded 4 months after the first recording of an event : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delay(df_mentions):\n",
    "    # Get delay between event time and mention time\n",
    "    \n",
    "    timeFmt = \"yyyy-MM-dd'T'HH:mm:ss.SSS\"\n",
    "    timeDiff = (unix_timestamp('MentionTimeDate', format=timeFmt) - unix_timestamp('EventTimeDate', format=timeFmt))\n",
    "    \n",
    "    return df_mentions.withColumn(\"Mention delay\", timeDiff)\n",
    "\n",
    "def restric_cov(df_mentions, days_threshold):\n",
    "    # Narrow down mentions to 2 month posterior to event mentions\n",
    "    \n",
    "    restric_index = df_mentions['Mention Delay'] <= days_threshold*24*3600\n",
    "    \n",
    "    return df_mentions[df_mentions.schema.names][restric_index].sort('GLOBALEVENTID')\n",
    "\n",
    "mentions = restric_cov(get_delay(mentions),120)\n",
    "mentions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_cov(df_mentions):\n",
    "    # Computing the mediatic coverage of each event in the mentions database\n",
    "    \n",
    "    return df_mentions.groupby('GLOBALEVENTID').agg(count('GLOBALEVENTID').alias('Number Mentions'))\n",
    "\n",
    "med_cov = get_media_cov(mentions.select('GLOBALEVENTID'))\n",
    "med_cov.select('Number Mentions').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the distribution of mediatic coverage in database\n",
    "med_cov = med_cov.toPandas()\n",
    "fig = plt.subplots(figsize=(10,4))\n",
    "plt.hist(med_cov['Number Mentions'],bins = med_cov['Number Mentions'].max())\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of relevant mentions per event')\n",
    "plt.ylabel('Frequency of occurence')\n",
    "plt.title('Distribution of mediatic coverage per event')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above, each row in the mentions database represents a mention of an event in a specific news report. Here we observe that the most frequent number of mentions is one, leading to the conclusion that a large amount of events receive very low mediatic coverage. However, we shall not forget that we are doing our analysis on a small subset of data and that we are missing out a certain amounts of mentions. There are however a few events which were mentioned quite a lot. Since we have data for the recording of mentions for 15 minutes each month, we can assume that the few events which lead to more mentions were widely reported right after they happened. They must be important events as they significantly raised the interest of the medias right after they happened. \n",
    "\n",
    "# Time\n",
    "After looking at the distributions of the mentions in the different sources, we would like to display the distribution of the mentions througout the months, to evaluate the media coverage. We would also like to display the distribution of the events reported in the medias througout the months, to evaluate the human activity. This will allow us to see how the data is distributed and to visualize any particular behavior with respect to time.\n",
    "\n",
    "### Human activity worldwide throughout the months\n",
    "We represent the distribution of human activity wordlwide throughout the months, that is the number of events taking place each month, according to the medias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORLDWIDE\n",
    "def get_events_worldwide(events_df):\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "    \n",
    "    events_worldwide =  events_df.groupBy('MonthYear_Date').count().orderBy('MonthYear_Date')\n",
    "    return events_worldwide.select([udf(column).alias('Month_Year') if column == 'MonthYear_Date' else column for column in events_worldwide.columns])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_worldwide_time = get_events_worldwide(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_worldwide_time = events_worldwide_time.toPandas()\n",
    "fig = plt.subplots(figsize=(20,5))\n",
    "plt.plot(events_worldwide_time['Month_Year'], events_worldwide_time['count'],marker='o')\n",
    "plt.xlabel('Time [months]')\n",
    "plt.ylabel('Number of events worldwide')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of events worldwide during 2 years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database contains events which are located prior in time to the beginning of the recording of mentions in the database, in February 2015. This in fact is due to the fact that mentions reference to events which just happened but also to events which happened in the past, and which will thus be registered in the database. However, since most of the events are referenced in the news right after they happened, these events will have less mentions than the ones which happened after the beginning of the recording. We thus choose to remove the events which happened prior to the beginning of the recording, in order to have an accurate idea of the total number of mentions per event.\n",
    "\n",
    "The last events recorded are in november 2017, which is consistent with the end time of the database we have access to. \n",
    "\n",
    "We now create a function which is going to remove the events which took place before the first mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_events (events_df) :\n",
    "\n",
    "    # first mention recording\n",
    "    first_record = datetime.strptime('20150218', '%Y%m%d').strftime('%Y-%m-%d')\n",
    "    # last event considered to have 2 months coverage for every event\n",
    "    last_considered_events = datetime.strptime('20170921', '%Y%m%d').strftime('%Y-%m-%d')\n",
    "    new_events = events_df.filter(events_df['date']>=first_record)\n",
    "    clean_new_events = new_events.filter(new_events['date']<= last_considered_events)\n",
    "    \n",
    "    return clean_new_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = clean_events(events)\n",
    "# check that the day of the first event is now the day of the first mention\n",
    "start = events.select('date').orderBy('date').head()\n",
    "end = events.select('date').orderBy(desc('date')).head()\n",
    "print(start)\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new representation of the events, once the events taking place before the recording of the first mentions have been removed\n",
    "events_worldwide_time = get_events_worldwide(events)\n",
    "events_worldwide_time = events_worldwide_time.toPandas()\n",
    "fig = plt.subplots(figsize=(20,5))\n",
    "plt.plot(events_worldwide_time['Month_Year'], events_worldwide_time['count'],marker='o')\n",
    "plt.xlabel('Time [months]')\n",
    "plt.ylabel('Number of events worldwide')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of events worldwide during 2 years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of mentions worldwide throughout the months\n",
    "We represent the distribution of the worldwide media coverage throughout the months, that is the number of mentions per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORLDWIDE\n",
    "udf_mention1 = UserDefinedFunction(lambda x: x.strftime('%Y%m'))\n",
    "udf_mention2 = UserDefinedFunction(lambda x: datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "# returns the number of mentions for each month, regardless of the countries\n",
    "def get_media_coverage_worldwide(mentions_df) :\n",
    "    \n",
    "    mentions_Year_Month = mentions_df.select([udf_mention1(column).alias('Year_Month_Mention') if column == 'MentionTimeDate' else column for column in mentions_df.columns])\n",
    "    mentions_year_month = mentions_Year_Month.groupBy('Year_Month_Mention').count().orderBy('Year_Month_Mention') \n",
    "    mentions_month_year = mentions_year_month.select([udf_mention2(column).alias('Month_Year_Mention') if column == 'Year_Month_Mention' else column for column in mentions_year_month.columns])\n",
    "    \n",
    "    return mentions_month_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_worldwide_time = get_media_coverage_worldwide(mentions.select('MentionTimeDate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_worldwide_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_worldwide_time = mentions_worldwide_time.toPandas()\n",
    "fig = plt.subplots(figsize=(20,5))\n",
    "plt.plot(mentions_worldwide_time['Month_Year_Mention'], mentions_worldwide_time['count'],marker='o')\n",
    "plt.xlabel('Time [months]')\n",
    "plt.ylabel('Number of mentions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of mentions worldwide during 2 years')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first mentions were in February 2015 and the last mentions were in November 2017, which is consistent with the time range of recording that we have access to, from February 18th 2015 at 11pm to November 23th 2017 at 8am. \n",
    "\n",
    "### Evolution of mentions for an event througout the months\n",
    "Now that we have had a rough overview of how events and mentions are distributed througout time, we would like to see how the mentions for a specific event are distributed througout time. We choose to select events which are mentioned a lot and which can give an actual idea of what happens across time, unlike small events which are only mentioned a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the 50 events which are the most mentioned during the 2 years\n",
    "largest_events = mentions.groupBy('GLOBALEVENTID').count().orderBy(desc('count')).limit(15)\n",
    "largest_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list = largest_events.select('GLOBALEVENTID').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_array = [int(i.GLOBALEVENTID) for i in ids_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_filtered = mentions.filter(col('GLOBALEVENTID').isin(ids_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_filtered.select('GLOBALEVENTID').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_mention1 = UserDefinedFunction(lambda x: x.strftime('%Y%m%d'))\n",
    "udf_mention2 = UserDefinedFunction(lambda x: datetime.strptime(x,'%Y%m%d').strftime('%d-%m-%Y'))\n",
    "\n",
    "# finds each mention of the most mentioned events\n",
    "largest_events_time = largest_events.select('GLOBALEVENTID').join(mentions_filtered.select('GLOBALEVENTID','MentionTimeDate'), 'GLOBALEVENTID')\n",
    "# finds the number of mentions per month for the most mentioned events (converts to a conveniable time format)\n",
    "largest_events_Year_Month = largest_events_time.select([udf_mention1(column).alias('Year_Month_Mention') if column == 'MentionTimeDate' else column for column in largest_events_time.columns])\n",
    "largest_events_year_month = largest_events_Year_Month.groupBy('Year_Month_Mention','GLOBALEVENTID').count().orderBy('Year_Month_Mention') \n",
    "largest_events_month_year = largest_events_year_month.select([udf_mention2(column).alias('Month_Year_Mention') if column == 'Year_Month_Mention' else column for column in largest_events_year_month.columns])\n",
    "    \n",
    "largest_events_month_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each event, looks whether its related mentions happened in the same month (not duplicated = FALSE) or in different month (duplicated = TRUE)\n",
    "largest_events_month_year.toPandas().duplicated(subset='GLOBALEVENTID', keep='first').unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the 50 events which are mentioned the most are mentioned only in the 15 minutes after which they were first mentioned. That is why their ID appear only one througout the largest_events_month_year dataframe, where the mentions for each events are stored during the months. However, for this descriptive analysis of the dataset, we are only considering 15 minutes of mentions per month. Thus, the most mentioned events probably have other mensions througout the year, but we are not aware of that because we only have a small subset of the data. To conclude, the results here are not significative. We would need data over a whole month to do a deeper analysis of the mentions of an event througout time. \n",
    "\n",
    "# Geography\n",
    "Now that we have seen the distribution of the mentions across time, we would like to have an idea of their geographic distribution in the different countries. This will give us an idea of the medias in the different countries, as well as giving us an insight on a query we would like to achieve in further analysis, that is whether the media coverage is biased by the country in which the event takes place.\n",
    "\n",
    "### Human activity per country (average over the 2 years)\n",
    "We represented the distribution of events in the different countries over the 2 years, to see in which countries there seems to be more events taking place, according to the information related in the medias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of events for each country\n",
    "def get_events_country(df_events) :\n",
    "    \n",
    "    return df_events.groupBy('ActionGeo_CountryCode').agg(count('GLOBALEVENTID').alias('human_activity')).orderBy('human_activity') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_country = get_events_country(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_country = events_country.toPandas()\n",
    "events_country.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the number of events of each country\n",
    "fig = plt.subplots(figsize=(2,4))\n",
    "events_country.boxplot(column='human_activity', grid=False)\n",
    "plt.title('Human activities related through the medias across countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_country.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier countries in terms of human activity in the countries over the 2 years\n",
    "outliers = events_country[(np.abs(events_country.human_activity-events_country.human_activity.mean()) > (3*events_country.human_activity.std()))]\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the human activity over the 2 years for the countries in which less events took place (LINEAR SCALE)\n",
    "events_country_less = events_country[events_country['human_activity']<33]\n",
    "x_pos = np.arange(len(events_country_less['human_activity']))\n",
    "fig, ax = plt.subplots(figsize=(22,5))\n",
    "ax.bar(x_pos, events_country_less['human_activity'])\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(events_country_less['ActionGeo_CountryCode'])\n",
    "ax.set_ylabel('Human activity')\n",
    "ax.set_title(\"Human activity per country over the 2 years (LINEAR SCALE)\")\n",
    "plt.xticks(x_pos, events_country_less['ActionGeo_CountryCode'], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the human activity over the 2 years for the countries in which more events took place \n",
    "# LOG SCALE because the US are a major outlier and prevent from seeing the other countries on a linear scale\n",
    "events_country_more = events_country[events_country['human_activity']>33]\n",
    "x_pos = np.arange(len(events_country_more['human_activity']))\n",
    "fig, ax = plt.subplots(figsize=(22,5))\n",
    "ax.bar(x_pos, events_country_more['human_activity'])\n",
    "ax.set_yscale('log')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(events_country_more['ActionGeo_CountryCode'])\n",
    "ax.set_ylabel('Human activity')\n",
    "ax.set_title(\"Human activity per country over the 2 years (LOG SCALE)\")\n",
    "plt.xticks(x_pos, events_country_more['ActionGeo_CountryCode'], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the informations related through the medias, the United States (US) seem to be the country where most of the human activity is taking place. Indeed, they are a clear outlier in the distribution of human activity, because their number of events is way above the median. Canada (CA) is also highly represented in terms of human activity. This might be because most of the news sources considered by the database come from the USA and will thus tend to display the events taking place in the country itself and in the bordering countries. \n",
    "\n",
    "What we also observe is that many events are associated with no country (None), which means that an important amount of events from the database are misclassified in terms of geographical location. However, since we have a huge amount of data, we chose to remove these events in the future, when we will represent features per country.\n",
    "\n",
    "We observe that according to the news, the countries which have less events taking place are countries from South America, Africa and East Europe (Roumania, Guatemala, Namibie, Djibouti, Mali, Honduras...). This might be because less medias report events from these countries or because the database doesn't have access to their medias. This might also just be because there are less events at the samples of time during which the recording were considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media coverage and mediatic attention per country (average over the 2 years)\n",
    "We looked at the distribution of media coverage and mediatic attention in the countries, to have an idea of the countries which are more present in the news. As a reminder, we consider the media coverage as the amount of mentions an event will receive in the 4 months following its recording. Here we look at the media coverage of all the events related to a country during the 2 years of recording. \n",
    "On the other hand, the mediatic attention (in percentage %) represents the attention an event receives in the medias on the wider scene. Here we look at the mediatic attention that the events of a country receive on the international scene, that is how much attention a country receives in the news, compared to another.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the media coverage for each country over the 2 years\n",
    "def get_media_coverage_country(events_df,mentions_df) :\n",
    "    \n",
    "    # mentions per event\n",
    "    mentions_count = mentions_df.groupBy('GLOBALEVENTID').count()\n",
    "    mentions_count1 = mentions_count.join(events_df, 'GLOBALEVENTID')\n",
    "    # mentions per country over the 2 years (= media coverage)\n",
    "    country_count = mentions_count1.groupBy('ActionGeo_CountryCode').agg(sum('count').alias('media_coverage')).orderBy(desc('media_coverage'))\n",
    "    # total number of mentions for all the events of the dataset which were recorded in the mentions dataset\n",
    "    total = mentions_df.join(events, 'GLOBALEVENTID').count()\n",
    "    # percentage of mentions per country over worldwide mentions over the 2 years (= mediatic attention)\n",
    "    country_count = country_count.withColumn('media_attention', col('media_coverage')/total)\n",
    "    country_count = country_count.withColumn('media_attention', col('media_attention')*100)\n",
    "    return country_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_countries = get_media_coverage_country(events, mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_countries = media_countries.toPandas()\n",
    "media_countries.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of the media attention of each country\n",
    "fig = plt.subplots(figsize=(2,4))\n",
    "media_countries.boxplot(column='media_attention', grid=False)\n",
    "plt.title('Media attention across countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_countries.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier countries in terms of media coverage over the 2 years\n",
    "outliers = media_countries[(np.abs(media_countries.media_attention-media_countries.media_attention.mean()) > (3*media_countries.media_attention.std()))]\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the media attention over the 2 years for the countries which got less media attention (LINEAR SCALE)\n",
    "media_country_less = media_countries[media_countries['media_attention']<0.054310]\n",
    "x_pos = np.arange(len(media_country_less['media_attention']))\n",
    "fig, ax = plt.subplots(figsize=(22,5))\n",
    "ax.bar(x_pos, media_country_less['media_attention'])\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(media_country_less['ActionGeo_CountryCode'])\n",
    "ax.set_ylabel('Media attention [%]')\n",
    "ax.set_title(\"Media attention per country over the 2 years (LINEAR SCALE)\")\n",
    "plt.xticks(x_pos, media_country_less['ActionGeo_CountryCode'], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the media attention over the 2 years for the countries which got more media attention (LINEAR SCALE)\n",
    "media_country_more = media_countries[media_countries['media_attention']>0.054310]\n",
    "x_pos = np.arange(len(media_country_more['media_attention']))\n",
    "fig, ax = plt.subplots(figsize=(22,5))\n",
    "ax.bar(x_pos, media_country_more['media_attention'])\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(media_country_more['ActionGeo_CountryCode'])\n",
    "ax.set_ylabel('Media attention [%]')\n",
    "ax.set_title(\"Media attention per country over the 2 years (LINEAR SCALE)\")\n",
    "plt.xticks(x_pos, media_country_more['ActionGeo_CountryCode'], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the median and the 3rd quartile for the distribution of the media coverage and attention are extremely low, but the means are higher. This shows that there are some countries which are highly represented in the news and which pull the mean to higher value. We see that a clear outlier in the data is the US. This might be because, as we wrote earlier, most of the news sources are american and will thus get an easier access to the news in the US and in the bordering countries than to the news in more distant countries. On top of having a high media coverage, the USA have a high mediatic attention, meaning that they have a high weigh on the international mediatic stage.\n",
    "\n",
    "We see that the countries which, according to the news, have the most events happening, are also the countries which get the most mediatic coverage and mediatic attention. For example United Kingdom (UK), Canada (CA), Australia (AU), China (CH), Pakistan (PK), Syria (SY) and Russsia (RS) are highly present in the medias, even though they are way behind the US. This is also consistent with the origins of the most prevalent media sources which are the UK, India, China and Australia. Indeed, these countries would naturally be more mentioned in the news if they have prominent news sources.\n",
    "\n",
    "# Type of Event Bias\n",
    "\n",
    "### Goldstein Ratio\n",
    "\n",
    "One of the goal of our project is to assess the bias of mediatic attention towards the type of an event: Is there a direct correlation between the type of event and the mediatic attention is receives?\n",
    "A way to perform this is to use the Goldstein ratio, which is a discrete numerical indicator between 10 and -10 that is directly linked to the event type. The Goldstein ratio assigned to each event type is directly linked to potential impact of this event on the country's stability : the larger the index, the more the event will lead to the strenghting of the country. The lower the index, the higher the chance that this event will weaken the country's stability.\n",
    "\n",
    "Before performing statistical correlation between the Goldstein ratio and the mediatic attention, let's explore the distribution of the Goldstein Ratio accross events reported by news during the 2 years. \n",
    "\n",
    "Documentation on goldstein ratio and cameo event types : \n",
    "- https://www.gdeltproject.org/data/lookups/CAMEO.eventcodes.txt\n",
    "- https://www.gdeltproject.org/data/lookups/CAMEO.goldsteinscale.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the distribution of the Goldstein Ratio accross events\n",
    "events.select('GoldsteinScale').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of events reported for each Goldstein ratio value\n",
    "def get_activity_byGoldsetin(df_events):\n",
    "    \n",
    "    total_event = df_events.count()\n",
    "    udf = UserDefinedFunction(lambda x: x/total_event, DoubleType())\n",
    "    \n",
    "    goldstein = df_events.groupby('GoldsteinScale').agg(count('GLOBALEVENTID').alias('Number Events')).orderBy('GoldsteinScale')\n",
    "    goldstein = goldstein.select([udf(column).alias('Fraction of Events') if column == 'Number Events' else column for column in goldstein.columns])\n",
    "    \n",
    "    return goldstein\n",
    "\n",
    "goldstein = get_activity_byGoldsetin(events).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldstein.plot.bar(x='GoldsteinScale', y='Fraction of Events', figsize=(8, 8), \\\n",
    "                   colormap='Paired',title = 'Fraction of events by Goldstein score ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the events displayed in the news are spread around a medium value on the Goldenstein scale, that is a value around 0. This means that these events should theoretically have no impact on a country's stability. What's more, most of the events have a score above zero. This leads us to believe that most of the human activity is leading toward the strengthening of the countries. However, eventhough few events are located between -10 and -5 on the scale, many reported events have a low Goldstein score of -10. Thus, there are more than 6% of the reported events which are destructive towards a country stability. We would like to investigate in our later analysis, whether this high percentage is accurately representative of an important amounts of negative events, or if it is just because these events are more related in the news.\n",
    "​\n",
    "Let's know get insight on the distribution of the mediatic coverage during the 2 years, regarding the Goldstein scale and therefore the event type :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the media coverage for each goldstein score\n",
    "def get_cov_index(df_events,df_mentions,index):\n",
    "    \n",
    "    # get_media_cov returns the number of mentions per event\n",
    "    df_mentions = get_media_cov(df_mentions).alias('mentions')\n",
    "    df_events = df_events.select(['GLOBALEVENTID',index]).alias('events')\n",
    "    cov_index = df_events.join(df_mentions, df_events['GLOBALEVENTID'] == df_mentions['GLOBALEVENTID'],how = 'left').select(['events.'+index,'mentions.*'])\n",
    "    cov_index = cov_index.where(cov_index[('GLOBALEVENTID')].isNotNull())\n",
    "    cov_index = cov_index.groupBy('GoldsteinScale').agg(sum('Number Mentions').alias('Number Mentions')).sort('GoldsteinScale')\n",
    "    \n",
    "    return cov_index\n",
    "\n",
    "goldstein2 = get_cov_index(events,mentions,'GoldsteinScale').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldstein2.plot.bar(x='GoldsteinScale', y='Number Mentions', figsize=(8, 8), colormap='Paired', \n",
    "                    title='Distribution of media coverage by Goldstein score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe here that the distribution of the mediatic coverage regarding the Goldstein score is very similar to the distribution of events regarding the Goldstein score. Indeed, most of the mentions are related to events with a Goldstein score around 0, in particular above 0 and a few mentions are related to events with score of -10. This would lead us to believe that the mentions are not related to the type of events, as they follow the same distribution as the events. Instead, the amount of mentions for a particular score seems to be associated to the amount of events for this score. We would like to do a deeper analysis of this relation in the next weeks. \n",
    "\n",
    "### QuadClass\n",
    "\n",
    "Another way to assess the bias of media coverage and attention depending on the event type is to consider another classification of events. The Quad Class feature classifies events into four global catagories :\n",
    "- Verbal cooperation\n",
    "- Material cooperation\n",
    "- Verbal conflict\n",
    "- Material conflict\n",
    "\n",
    "In order to have an idea of the distribution of events regarding the quad class in our dataset, we represented the percentage of events in each class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the distribution of the Quad Class type accross events\n",
    "quad_val = ['Verbal Cooperation','Material Cooperation','Verbal Conflict','Material Conflict']\n",
    "events.select('QuadClass').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(labels):\n",
    "    \n",
    "    return [x for i,x in enumerate(quad_val) if (i+1) in labels]\n",
    "\n",
    "def get_quad(df_events):\n",
    "    \n",
    "    return df_events.groupby('QuadClass').agg(count('GLOBALEVENTID').alias('Number Events')).orderBy('QuadClass')\n",
    "    \n",
    "def get_piechart_data(df,str1,str2):\n",
    "    \n",
    "    one_list = list(df.select(str1).toPandas()[str1])\n",
    "    two_list = list(df.select(str2).toPandas()[str2])\n",
    "\n",
    "    return pd.DataFrame({str1: one_list},index= two_list)\n",
    "\n",
    "quad = get_piechart_data(get_quad(events),'Number Events','QuadClass')\n",
    "quad.plot.pie(y='Number Events', autopct='%.2f',figsize=(5, 5),labels=get_class(list(quad.index)),title = 'Quad Class type distribution in events')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the majority of reported events are related to Verbal Cooperation, the distribution between the four Quad classes is therefore not homogene. Thus, most activities related by the news are not conflictual. This is consistent with what we observed regarding the Goldstein score, where most of the reported events have a score above 0. We see though that more than 25% of the events related by the news are conflictual. \n",
    "\n",
    "The media attention index considered later on in our analysis should therefore take into consideration the inhomogenity of the distribution. The media coverage (resp. attention) should be a ratio between the number of mentions (resp. normalised number of mentions) per QuadClass type and the total number of events per QuadClass type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cov_quad(df_events,df_mentions):\n",
    "    \n",
    "    df_mentions = get_media_cov(df_mentions).alias('mentions')\n",
    "    df_events = df_events.alias('events')\n",
    "    cov_quad = df_events.join(df_mentions, df_events['GLOBALEVENTID'] == df_mentions['GLOBALEVENTID'],how = 'left').select(['events.QuadClass','mentions.*']).sort('GLOBALEVENTID')\n",
    "\n",
    "    return cov_quad\n",
    "\n",
    "med_cov_byQuadClass = get_cov_quad(events,mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad = med_cov_byQuadClass.groupby('QuadClass').agg(sum(med_cov_byQuadClass['Number Mentions']).alias('Number Mentions')).orderBy('QuadClass')\n",
    "\n",
    "quad = get_piechart_data(quad,'Number Mentions','QuadClass')\n",
    "quad.plot.pie(y='Number Mentions', autopct='%.2f',figsize=(5, 5),labels=get_class(list(quad.index)),title = 'Quad Class type distribution in relevant mentions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that once again, the distribution of mentions regarding the Quad class of the reported events is similar to the distribution of events regarding the Quad class. Thus, this would mean that the number of mentions regarding the Quad class is related to the number of events in this quad class and not to the quad class itself. We would like to further analyze with the rest of the dataset any association between media coverage, events and type of events in the next weeks, in order to determine whether there is any relation between the media coverage and the quad class, that is, if an event belonging to a certain class will be given more importance by the medias compared to another class.\n",
    "\n",
    "### Violent and Peaceful Events \n",
    "\n",
    "On top of the Goldstein score and the quad class classification, the Event Root Code provides information on the specific type of an event. It stores this information in a CAMEO format which is a numerical values related to a specific category of event. The categories are PUBLIC STATEMENT, APPEAL, EXPRESS, COOPERATE, CONSULT, DIPLOMATIC COOPERATION, PROVIDE AID, YIELD, INVESTIGATE, DEMAND, DISAPPROVE, REJECT, THREATEN, PROTEST, FORCE POSTURE, REDUCE RELATIONS, COERCE, ASSAULT, FIGHT and MASS VIOLENCE. We represented the distribution of the events regarding their Event Root Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val = ['PUBLIC STATEMENT','APPEAL', 'INTENT TO COOPERATE','CONSULT','DIPLOMATIC COOPERATION','MATERIAL COOPERATION',\n",
    "          'PROVIDE AID','YIELD','INVESTIGATE','DEMAND','DISAPPROVE','REJECT','THREATEN','PROTEST','FORCE POSTURE',\n",
    "          'REDUCE RELATIONS','COERCE','ASSAULT','FIGHT','MASS VIOLENCE']\n",
    "\n",
    "# returns the proportion of events which are in each category of events\n",
    "def get_activity_byType(df_events):\n",
    "\n",
    "    total_event = df_events.count()\n",
    "    udf = UserDefinedFunction(lambda x: x/total_event, DoubleType())\n",
    "\n",
    "    root_type = df_events.groupby('EventRootCode').agg(count('GLOBALEVENTID').alias('Number of Events')).sort('EventRootCode')\n",
    "    root_type = root_type.select([udf(column).alias('Percentage of Events') if column == 'Number of Events' else column for column in root_type.columns])\n",
    "\n",
    "    return root_type \n",
    "\n",
    "# returns the label for the numerical values of the Event Root Code \n",
    "def get_reallabels(df):\n",
    "    \n",
    "    real_labels = df['EventRootCode'].values\n",
    "    labels = [labels_val[int(x)-1] for x in real_labels]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "act_byType = get_activity_byType(events).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = np.arange(len(act_byType['EventRootCode']))\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "ax.bar(x_pos, act_byType['Percentage of Events'])\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_ylabel('Proportion of events')\n",
    "ax.set_title('Distribution of event types')\n",
    "plt.xticks(x_pos, get_reallabels(act_byType), rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sample of recorded events which we considered are mostly neutral events, with no violent character. Indeed, most of the events are consultation, public statement, intent to cooperate and diplomatic cooperation. Whereas less events are related to assault, threats and mass violence. However, almost 10% of the events are related to fight. Thus, most events reported by the news seem to be leading to better world conditions.\n",
    "\n",
    "# Let's now concentrate on some countries.... \n",
    "\n",
    "We now want to observe different features for specific countries, in order to have an idea of the localized situation. We would like to know if we can already observe a relation between these features. We choose to do this analysis for :\n",
    "- the USA, as they are highly prominent in the media and seem to be really active in terms of human activity, according to what is reported through the news\n",
    "- Syria, as it is a country in war and would thus be expected to be unstable and present in the medias\n",
    "- Pakistan, as it has been the target of terroristic attacks in the recent years and would also be expected to be the subject of the news\n",
    "- Australia, as it is an occidental country in the South hemisphere\n",
    "\n",
    "We thus looked throughout the months at the events, the media coverage and the Goldstein score. We also looked for these 4 countries at their distribution of event types across the 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives the average Goldstein ration per month\n",
    "def get_Goldstein(df) :\n",
    "\n",
    "    udf = UserDefinedFunction(lambda x: datetime.strptime(x,'%Y%m').strftime('%m-%Y'))    \n",
    "    df_Goldstein = df.groupBy('MonthYear_Date').agg(mean('GoldsteinScale').alias('av_Goldstein')).orderBy('MonthYear_Date')\n",
    "    return df_Goldstein.select([udf(column).alias('Month_Year') if column == 'MonthYear_Date' else column for column in df_Goldstein.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events related to these countries\n",
    "events_US = events.filter(events['ActionGeo_CountryCode'] == 'US')\n",
    "events_SY = events.filter(events['ActionGeo_CountryCode'] == 'SY')\n",
    "events_PK = events.filter(events['ActionGeo_CountryCode'] == 'PK')\n",
    "events_AU = events.filter(events['ActionGeo_CountryCode'] == 'AU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mentions related to these countries\n",
    "mentions_US = events_US.join(mentions, 'GLOBALEVENTID')\n",
    "mentions_SY = events_SY.join(mentions, 'GLOBALEVENTID')\n",
    "mentions_PK = events_PK.join(mentions, 'GLOBALEVENTID')\n",
    "mentions_AU = events_AU.join(mentions, 'GLOBALEVENTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human activity per month\n",
    "events_US_time = get_events_worldwide(events_US) \n",
    "events_SY_time = get_events_worldwide(events_SY) \n",
    "events_PK_time = get_events_worldwide(events_PK) \n",
    "events_AU_time = get_events_worldwide(events_AU) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media coverage per month\n",
    "mentions_US_time = get_media_coverage_worldwide(mentions_US)\n",
    "mentions_SY_time = get_media_coverage_worldwide(mentions_SY)\n",
    "mentions_PK_time = get_media_coverage_worldwide(mentions_PK)\n",
    "mentions_AU_time = get_media_coverage_worldwide(mentions_AU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goldstein per month\n",
    "Goldstein_US = get_Goldstein(events_US)\n",
    "Goldstein_SY = get_Goldstein(events_SY)\n",
    "Goldstein_PK = get_Goldstein(events_PK)\n",
    "Goldstein_AU = get_Goldstein(events_AU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all Spark dataframes to Pandas dataframes for plotting\n",
    "events_US_time_pd = events_US_time.toPandas()\n",
    "events_SY_time_pd = events_SY_time.toPandas()\n",
    "events_PK_time_pd = events_PK_time.toPandas()\n",
    "events_AU_time_pd = events_AU_time.toPandas()\n",
    "\n",
    "mentions_US_time_pd = mentions_US_time.toPandas()\n",
    "mentions_SY_time_pd = mentions_SY_time.toPandas()\n",
    "mentions_PK_time_pd = mentions_PK_time.toPandas()\n",
    "mentions_AU_time_pd = mentions_AU_time.toPandas()\n",
    "\n",
    "Goldstein_US_pd = Goldstein_US.toPandas()\n",
    "Goldstein_SY_pd = Goldstein_SY.toPandas()\n",
    "Goldstein_PK_pd = Goldstein_PK.toPandas()\n",
    "Goldstein_AU_pd = Goldstein_AU.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time [months]')\n",
    "plt.xticks(rotation=45)\n",
    "ax1.set_ylabel('Number of events', color=color)\n",
    "ax1.plot(events_US_time_pd['Month_Year'], events_US_time_pd['count'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Number of mentions', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(mentions_US_time_pd['Month_Year_Mention'], mentions_US_time_pd['count'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax3 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:green'\n",
    "ax3.set_ylabel('Average goldstein ratio', color=color)  # we already handled the x-label with ax1\n",
    "ax3.plot(Goldstein_US_pd['Month_Year'], Goldstein_US_pd['av_Goldstein'], color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "ax3.spines['right'].set_position(('outward', 60)) \n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('Evolution of the human activity, media coverage and average Golstein ratio in the USA')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time [months]')\n",
    "plt.xticks(rotation=45)\n",
    "ax1.set_ylabel('Number of events', color=color)\n",
    "ax1.plot(events_SY_time_pd['Month_Year'], events_SY_time_pd['count'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Number of mentions', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(mentions_SY_time_pd['Month_Year_Mention'], mentions_SY_time_pd['count'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax3 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:green'\n",
    "ax3.set_ylabel('Average goldstein ratio', color=color)  # we already handled the x-label with ax1\n",
    "ax3.plot(Goldstein_SY_pd['Month_Year'], Goldstein_SY_pd['av_Goldstein'], color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "ax3.spines['right'].set_position(('outward', 60)) \n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('Evolution of the human activity, media coverage and average Golstein ration in Syria ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time [months]')\n",
    "plt.xticks(rotation=45)\n",
    "ax1.set_ylabel('Number of events', color=color)\n",
    "ax1.plot(events_PK_time_pd['Month_Year'], events_PK_time_pd['count'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Number of mentions', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(mentions_PK_time_pd['Month_Year_Mention'], mentions_PK_time_pd['count'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax3 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:green'\n",
    "ax3.set_ylabel('Average goldstein ratio', color=color)  # we already handled the x-label with ax1\n",
    "ax3.plot(Goldstein_PK_pd['Month_Year'], Goldstein_PK_pd['av_Goldstein'], color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "ax3.spines['right'].set_position(('outward', 60)) \n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('Evolution of the human activity, media coverage and average Golstein ratio in Pakistan ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time [months]')\n",
    "plt.xticks(rotation=45)\n",
    "ax1.set_ylabel('Number of events', color=color)\n",
    "ax1.plot(events_AU_time_pd['Month_Year'], events_AU_time_pd['count'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Number of mentions', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(mentions_AU_time_pd['Month_Year_Mention'], mentions_AU_time_pd['count'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax3 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:green'\n",
    "ax3.set_ylabel('Average goldstein ration', color=color)  # we already handled the x-label with ax1\n",
    "ax3.plot(Goldstein_AU_pd['Month_Year'], Goldstein_AU_pd['av_Goldstein'], color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "ax3.spines['right'].set_position(('outward', 60)) \n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('Evolution of the human activity, media coverage and average Golstein ratio in Australia ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these four countries, we observe certain behaviours across time. \n",
    "We see that in the US, there is a clear relation between the number of events and the media coverage each month, as the 2 curves are very close to each other. We also see that the peaks in the reported events happen simultaneously with the peaks in the average Goldstein score. This displays an interesting relation, where more events lead to more media coverage, but also where more reported events mean a slighlty higher Goldstein score, which could indicate that there are more news when the events are less neutral.\n",
    "\n",
    "For Syria, Pakistan and Australia, there are significantly less reported events and less mentions compared to the US. In these countries, the number of mentions seems to be related to the number of events as well, as the two curves follow similar evolutions through time. We observe that Syria and Pakistan have the fewer numbers of reported events and mentions througout the months, although Syria is at war and Pakistan is the center of many conflicts due to political and military unstability, as well as the terroristic attacks by the Taliban Pakistan. This unstability is reflected by the evolution of the Goldstein score thoughout time, which tends to often go below 0. Australia however has an average Goldstein score which is maintained between 0 and 2, although it peaks downard for a few occasions. It is interesting to see that countries which are subject to violent conflicts are equally or less present in the medias compared to countries like Australia which would be considered more stable and in particular like the US.\n",
    "\n",
    "After looking at the evolution througout the months, we looked at the average situation over the 2 years. We looked at the distribution of violent and peaceful events in the four countries, averaged over the 2 months. We considered ad violent the events with an Event Root Code above 18 (18 = Assault, 19 = Fight, 20 = Mass violence) and as peaceful the ones with a code below 3 (1 = Public statement, 2 = Appeal, 3= Expression of intent to cooperate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_violentevents(df_events):\n",
    "    \n",
    "    violent_index = df_events['EventRootCode'] >= 18\n",
    "    \n",
    "    return df_events[violent_index]\n",
    "\n",
    "violent = get_violentevents(events)\n",
    "\n",
    "def get_peacefullevents(df_events):\n",
    "    \n",
    "    peace_index = df_events['EventRootCode'] <= 3\n",
    "    \n",
    "    return df_events[peace_index]\n",
    "\n",
    "peace = get_peacefullevents(events)\n",
    "extremes = peace.union(violent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_byTypeCountry(df_events):\n",
    "    \n",
    "    total_event = df_events.count()\n",
    "    udf = UserDefinedFunction(lambda x: x/total_event, DoubleType())\n",
    "    \n",
    "    count_type = df_events.groupby('ActionGeo_CountryCode','EventRootCode').agg(count('GLOBALEVENTID').alias('Number of Events')).orderBy('ActionGeo_CountryCode','EventRootCode')    \n",
    "    count_type = count_type.select([udf(column).alias('Percentage of Events') if column == 'Number of Events' else column for column in count_type.columns])\n",
    "\n",
    "    return count_type\n",
    "\n",
    "type_count = get_activity_byTypeCountry(extremes).toPandas()\n",
    "type_count = type_count.pivot(index='ActionGeo_CountryCode', columns='EventRootCode', values='Percentage of Events')\n",
    "type_count = type_count.loc[['US','SY','PK','AU']]\n",
    "type_count.plot.bar(figsize = (22,10),stacked=True,logy=True, title = 'Distribution of event types in the USA, SYRIA, PAKISTAN,AUSTRALIA in log scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1: Public statement\n",
    "- 2: Appeal\n",
    "- 3: Expression of intent to cooperate\n",
    "- 18: Assault\n",
    "- 19: Fight\n",
    "- 20: Use of unconventional mass violence\n",
    "\n",
    "Considering the logarithmic scale on this bar chart, as displayed above, the US have far more reported events than the other countries and the majority of these events are pacific (public statement, appeal, expression of intent to cooperate). Eventhough less events were reported for Australia, these events were mostly pacific as well. On the other hand, Syria and Pakistan have an important proportion of violent events, which reflects what was displayed above by their average Goldstein scores. Indeed, their scores were fluctuating towards low values during the 2 years, indicating unstability. Looking at this, we are impressed by the amount of events reported by the medias in the US, eventhough the country appears to be stable. Whereas the activity of countries which are in critical situations like Syria and Pakistan and which would tend to be more interesting regarding their situation, is not so much reported in the medias.\n",
    "\n",
    "After these primary observations of our data and of the behaviour of our data regarding certain features, we would like to do a deeper analysis using the whole dataset, in order to identify whether the different features are correlated, for example whether we can predict the state or stability of a country by looking at the media coverage. Thus we would like to identify whether it is accurate to say that medias are biases, by taking in consideration the fact that the medias which were reported in this database tend to mostly be american, and thus might not represent the whole picture of media coverage in the different countries.\n",
    "\n",
    "# Cluster integration\n",
    "\n",
    "Aside from exploring a local subset of datas, we also run a small query on the cluster to test our setup.\n",
    "The computation took quite a long time, although our query was only to compute the sum of all mentions for avery country during the time period covered by the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mentions_per_country_fipsCode.csv was generated on the cluster and downloaded manually\n",
    "mentionsPerCountry = spark.read.csv('mentions_per_country_fipsCode.csv', schema=StructType([\n",
    "    StructField(\"alpha-2\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), True)]))\n",
    "\n",
    "mentionsPerCountry = mentionsPerCountry.toPandas()\n",
    "mentionsPerCountry = mentionsPerCountry.sort_values('count', ascending=False)\n",
    "mentionsPerCountry['alpha-2'] = mentionsPerCountry['alpha-2'].apply(lambda x: fips2iso(x))\n",
    "mentionsPerCountry.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_topo = json.load(open('world-countries-sans-antarctica.json'))\n",
    "\n",
    "\n",
    "world_map = folium.Map(location=[30, 0], tiles='Mapbox Bright', zoom_start=1.5)\n",
    "world_map.choropleth(geo_data=world_topo,\n",
    "                     data=mentionsPerCountry,\n",
    "                     columns=['alpha-2', 'count'],\n",
    "                     topojson='objects.countries1',\n",
    "                     fill_color='OrRd', fill_opacity=0.7,\n",
    "                     key_on = 'properties.Alpha-2',\n",
    "                  threshold_scale=[0, 24000, 240000, 2400000, 24000000, 240000000])\n",
    "\n",
    "\n",
    "\n",
    "world_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILESTONE 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each new function was first tried with a sample of the data, as for the second milestone. They were then run on the cluster with the whole dataset and visualized (see the results below).\n",
    "\n",
    "We wanted to see what kind of information a mediatic source was relaying to the inhabitants of the country where it was written. Indeed, we wanted to see whether a media mostly transmits information about the country where it is written or if it displays informations about foreign countries. Thus we selected a few highly represented media sources and for each of them, we looked at :\n",
    "- the number of times they mentioned each country, with respect to all the mentions of this media\n",
    "- the number of events they mentioned in each country, with respect to the number of events which took place in the country during the 2 years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of mentions per country per source\n",
    "def mentions_biggest_sources1(df_mentions,df_events,selected_sources) :\n",
    "    \n",
    "    mentions_selected_sources = df_mentions.filter(col('MentionSourceName').isin(selected_sources))\n",
    "    mentions_selected_sources = mentions_selected_sources.groupBy('MentionSourceName','GLOBALEVENTID').agg(count('GLOBALEVENTID').alias('Number_mentions_event'))\n",
    "    mentions_selected_sources = mentions_selected_sources.join(df_events, 'GLOBALEVENTID').select('MentionSourceName','GLOBALEVENTID','Number_mentions_event','ActionGeo_CountryCode')\n",
    "    return mentions_selected_sources.groupBy('MentionSourceName','ActionGeo_CountryCode').agg(sum('Number_mentions_event').alias('Number_Mentions'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of events mentioned per country per source, and the total number of events in each country\n",
    "def mentions_biggest_sources2(df_mentions,df_events,selected_sources) :\n",
    "    \n",
    "    mentions_selected_sources = df_mentions.filter(col('MentionSourceName').isin(selected_sources))\n",
    "    # for each sources finds the IDs of the events it mentiones\n",
    "    mentions_selected_sources = mentions_selected_sources.groupBy('MentionSourceName','GLOBALEVENTID').count().select('MentionSourceName','GLOBALEVENTID')\n",
    "    # find the country for each of these events\n",
    "    mentions_selected_sources = mentions_selected_sources.join(df_events, 'GLOBALEVENTID').select('MentionSourceName','GLOBALEVENTID','ActionGeo_CountryCode')\n",
    "    # finds the overall number of events for each country in the 2 years\n",
    "    events_country = df_events.groupBy('ActionGeo_CountryCode').agg(count('GLOBALEVENTID').alias('Number_events_country'))\n",
    "    # for each country mentioned in the sources, associates its number of events in the 2 years\n",
    "    sources_events = mentions_selected_sources.join(events_country,'ActionGeo_CountryCode').select('MentionSourceName','GLOBALEVENTID','ActionGeo_CountryCode','Number_events_country')\n",
    "    # finds the number of events in each country mentioned by theses specific media sources\n",
    "    return sources_events.groupBy('MentionSourceName','ActionGeo_CountryCode','Number_events_country').agg(count('GLOBALEVENTID').alias('Number_events_source'))\n",
    "                                                                                                                                                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_sources_selection = list(['dailymail.co.uk','couriermail.com.au','abc-7.com','english.wafa.ps'])\n",
    "mentions_selected_sources1 = mentions_biggest_sources1(mentions.select('MentionSourceName', 'GLOBALEVENTID'),events.select('GLOBALEVENTID','ActionGeo_CountryCode'),biggest_sources_selection)\n",
    "mentions_selected_sources2 = mentions_biggest_sources2(mentions.select('MentionSourceName', 'GLOBALEVENTID'),events.select('GLOBALEVENTID','ActionGeo_CountryCode'),biggest_sources_selection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of times they mention each country : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_selected_sources1.orderBy(['MentionSourceName','ActionGeo_CountryCode']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected Daily Mail UK and looked, for each country, at the percentage of mentions abouth this country published by Daily Mail UK with respect to the number of mentions published by Daily Mail UK in total :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_mentions_biggest_sources_UK = mentions_selected_sources1.filter(mentions_selected_sources1['MentionSourceName']=='dailymail.co.uk').toPandas()\n",
    "countries_mentions_biggest_sources_UK['Percentage_mentions']=(countries_mentions_biggest_sources_UK['Number_Mentions']*100)/countries_mentions_biggest_sources_UK['Number_Mentions'].sum()\n",
    "countries_mentions_biggest_sources_UK.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each source the number of events they mentioned in each country (Number_events_source), with respect to the number of events which took place in the country during the 2 years (Number_events_country) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_selected_sources2.orderBy(['MentionSourceName','ActionGeo_CountryCode']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_selected_sources2 = mentions_selected_sources2.toPandas()\n",
    "mentions_selected_sources2['Percentage_events'] = (mentions_selected_sources2['Number_events_source']*100)/mentions_selected_sources2['Number_events_country']\n",
    "mentions_selected_sources2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When later evaluating this function on the cluster, we selected these mediatic sources :  \n",
    "\n",
    "- sources france24.com \n",
    "- washingtonpost.com \n",
    "- theguardian.com\n",
    "- thejakartapost.com\n",
    "- thehindu.com\n",
    "- dailytelegraph.com.au\n",
    "- gulfnews.com\n",
    "- japantimes.co.jp\n",
    "- chinadaily.com.cn\n",
    "- timesofisrael.com\n",
    "- rt.com \n",
    "- kenyastar.com\n",
    "\n",
    "We then looked again at the Goldstein score. We had already evaluated the number of events and the number of mentions worldwide for each Goldstein score. We then wanted to evaluate, instead of the number of mentions, the average number of mentions per event for each Goldstein score. We thought it would be more relevant than the number of mentions itsef, as it represents the average importance given to a category of event :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Goldstein_mediaCov(df_mentions,df_events) : \n",
    "    \n",
    "    media_coverage = get_media_cov(df_mentions)\n",
    "    df =  media_coverage.join(df_events,'GLOBALEVENTID').select('GoldsteinScale','Number Mentions')\n",
    "    return df.groupBy('GoldsteinScale').agg(avg('Number mentions').alias('Average media coverage per event'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Goldstein_mediaCov(mentions.select('GLOBALEVENTID'),events.select('GLOBALEVENTID','GoldsteinScale')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow this idea of giving more weight to certains categories of event, we computed for each event its media coverage (count) to compare it with its Goldstein score, in order to observe whether there is any relationship between the two :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_cov(df_mentions, df_events):\n",
    "\n",
    "    # Computing the mediatic coverage of each event in the mentions database\n",
    "    goldstein = df_events.select('GLOBALEVENTID', 'GoldsteinScale')\n",
    "    ret = df_mentions.groupby('GLOBALEVENTID').count()\n",
    "    return ret.join(goldstein, 'GLOBALEVENTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_media_cov(mentions, events).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then wanted to calculate the number of events for each country for each month, to see the evolution of the human activity on a map through time :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_per_country(events_df):\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "    \n",
    "    events_worldwide =  events_df.groupBy('MonthYear_Date','ActionGeo_CountryCode').agg(count('GLOBALEVENTID').alias('Number_Events')).orderBy(['MonthYear_Date','Number_Events'],ascending=[True,True])\n",
    "    return events_worldwide.select([udf(column).alias('Month_Year') if column == 'MonthYear_Date' else column for column in events_worldwide.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_country_time = get_events_per_country(events)\n",
    "events_country_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did the same but we loooked at the media coverage, that is here the number of mentions about each country each month : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the media coverage for each country for each month\n",
    "def get_media_cov_per_country(df_events,df_mentions):\n",
    "    \n",
    "    df = df_mentions.join(df_events,'GLOBALEVENTID').select('GLOBALEVENTID','ActionGeo_CountryCode','MentionTimeDate')\n",
    "    mentions_Year_Month = df.select(\n",
    "        [udf_mention1(column).alias('Year_Month_Mention') if column == 'MentionTimeDate' else column for column in\n",
    "         df.columns])    \n",
    "    mentions_year_month = mentions_Year_Month.groupBy('Year_Month_Mention','ActionGeo_CountryCode').count().orderBy('Year_Month_Mention')\n",
    "    mentions_month_year = mentions_year_month.select(\n",
    "        [udf_mention2(column).alias('Month_Year_Mention') if column == 'Year_Month_Mention' else column for column in\n",
    "         mentions_year_month.columns])\n",
    "    return mentions_month_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_media_cov_per_country(events.select('GLOBALEVENTID', 'ActionGeo_CountryCode'),mentions.select('GLOBALEVENTID', 'MentionTimeDate')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to look specifically at what is happening in a few selected countries (Syria, USA, Australia, Pakistan), we evaluated for each of these countries the average media coverage for peaceful events (public statement 01, appeal 02, expression to intent to cooperate 03), as well as for violent events (Assault 18, Fight 19, Use of unconventionnal mass violence 20), all months considered :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_media_coverage_country_type(df_mentions,df_events) :\n",
    "    \n",
    "    violent = get_violentevents(df_events)\n",
    "    peace = get_peacefullevents(df_events)\n",
    "    df = peace.union(violent)\n",
    "    df1 = df.join(df_mentions,'GLOBALEVENTID').select('GLOBALEVENTID','ActionGeo_CountryCode','EventRootCode')\n",
    "    df2 = df1.groupBy('ActionGeo_CountryCode','EventRootCode','GLOBALEVENTID').agg(count('GLOBALEVENTID').alias('Number_mentions'))\n",
    "    return df2.groupBy('ActionGeo_CountryCode','EventRootCode').agg(mean('Number_mentions').alias('Average media coverage per event'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_spe_countries = events.select('ActionGeo_CountryCode','GLOBALEVENTID','EventRootCode').filter(col('ActionGeo_CountryCode').isin(list(['US', 'AU', 'SY', 'PK'])))    \n",
    "events_spe_countries = get_av_media_coverage_country_type(mentions,events_spe_countries).orderBy('ActionGeo_CountryCode','EventRootCode')\n",
    "events_spe_countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these specific countries, we also looked at the pacific and violent human activities throughout the months :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_byTypeCountry_time(df_events):\n",
    "    violent = get_violentevents(df_events)\n",
    "    peace = get_peacefullevents(df_events)\n",
    "    df = peace.union(violent)\n",
    "    df_new = df.groupby('ActionGeo_CountryCode','MonthYear_Date','EventRootCode').agg(count('GLOBALEVENTID').alias('Number of Events')).orderBy('ActionGeo_CountryCode', 'MonthYear_Date','EventRootCode')\n",
    "    return df_new.select(\n",
    "        [udf_mention2(column).alias('Month_Year') if column == 'MonthYear_Date' else column for column in df_new.columns])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_spe_countries = events.filter(col('ActionGeo_CountryCode').isin(list(['US','AU','SY','PK'])))\n",
    "type_events_countries = get_activity_byTypeCountry_time(events_spe_countries.select('GLOBALEVENTID','ActionGeo_CountryCode','MonthYear_Date', 'EventRootCode'))\n",
    "type_events_countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results vizualisation\n",
    "We evaluated the functions designed for milestone 2 on the cluster and we visualized the results. We also visualized the results of the functions designed in milestone 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = pd.read_csv('./data/get_confidence.csv', header = 0)\n",
    "confidence = confidence.sort_values('Confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace0 = go.Bar(\n",
    "    x=confidence['Confidence'].apply(str),\n",
    "    y=confidence['count'],\n",
    "    text=confidence['count'],\n",
    "    marker=dict(\n",
    "        color='#AFEEEE',\n",
    "        line=dict(\n",
    "            color='#AFEEEE',\n",
    "            width=1.5,\n",
    "                )\n",
    "                ),\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "data = [trace0]\n",
    "layout = go.Layout(\n",
    "    title='Number of Mentions by Confidence Index',\n",
    "    xaxis=dict(\n",
    "        title='Confidence Index',\n",
    "        autorange=True,\n",
    "        showgrid=False,\n",
    "\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Number of Mentions',\n",
    "        autorange=True,\n",
    "        showgrid=False,\n",
    "    ),\n",
    "    width=700,\n",
    "    height=500,\n",
    "    \n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Confidence_Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type of sources Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_type = pd.read_csv('./data/get_sources.csv', header = 0)\n",
    "sources_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = {\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"values\": sources_type['Number Mentions'],\n",
    "      \"labels\": ['Web Sources','Offline Sources'],\n",
    "      #\"text\":'none',\n",
    "      #\"textposition\":\"inside\",\n",
    "      \"domain\": {\"x\": [0, 1]},\n",
    "      \"name\": \"Type\",\n",
    "      \"hole\": .4,\n",
    "      \"type\": \"pie\",\n",
    "      \"marker\" : {\"colors\":['#AFEEEE', '#1A76FF']}\n",
    "    }],\n",
    "  \"layout\": {\n",
    "        \"title\":\"Sources Type GDELT 2.0 Dataset\"\n",
    "    }\n",
    "}\n",
    "py.iplot(fig, filename='Source_Type_Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goldstein  scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we represent the number of events and the average media coverage per event for each Goldstein score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a changer pour avoir average & nb events\n",
    "mentions_Golstein_data = pd.read_csv('./data/get_cov_index.csv')\n",
    "events_Golstein_data = pd.read_csv('./data/get_cov_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_Golstein_data = mentions_Golstein_data.set_index(mentions_Golstein_data['GoldsteinScale'].astype(str))\n",
    "events_Golstein_data = events_Golstein_data.set_index(events_Golstein_data['GoldsteinScale'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_Golstein_data['GoldsteinScale'] = mentions_Golstein_data['GoldsteinScale'].astype(float)\n",
    "events_Golstein_data['GoldsteinScale'] = events_Golstein_data['GoldsteinScale'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in enumerate(events_Golstein_data.index.values):\n",
    "    events_Golstein_data.index.values[i] = events_Golstein_data.index.values[i] + '`'\n",
    "    mentions_Golstein_data.index.values[i] = mentions_Golstein_data.index.values[i] + '`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Bar(\n",
    "    #x=np.linspace(mentions_Golstein_data['GoldsteinScale'].min(),mentions_Golstein_data['GoldsteinScale'].max(),len(mentions_Golstein_data['GoldsteinScale'].values)),\n",
    "    x = events_Golstein_data.index,\n",
    "    y=mentions_Golstein_data['Number Mentions'],\n",
    "    text='none',\n",
    "    name = 'Average Media Coverage',\n",
    "    marker=dict(\n",
    "        color='rgb(26, 118, 255)',\n",
    "        line=dict(\n",
    "            color='rgb(26, 118, 255)',\n",
    "            width=1.5),\n",
    "    opacity=0.6),\n",
    "    #yaxis='y1',\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    #x=np.linspace(events_Golstein_data['GoldsteinScale'].min(),events_Golstein_data['GoldsteinScale'].max(),len(events_Golstein_data['GoldsteinScale'].values)),\n",
    "    x = mentions_Golstein_data.index,\n",
    "    y=events_Golstein_data['Number Mentions'],\n",
    "    text='none',\n",
    "    name='Percentage of Total Events',\n",
    "    marker=dict(\n",
    "        color='rgb(175,238,238)',\n",
    "        line=dict(\n",
    "            color='rgb(175,238,238)',\n",
    "            width=1.5),\n",
    "    opacity=0.6),\n",
    "    #yaxis='y2'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Human Activity & Mediatic Coverage by Goldstein Index',\n",
    "    xaxis=dict(\n",
    "        #ticks = 'hello',\n",
    "        tickangle=-45,\n",
    "        tickfont=dict(\n",
    "            size=14,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        )\n",
    "    ),\n",
    "    yaxis1=dict(\n",
    "        title='Average Number of Mentions',\n",
    "        titlefont=dict(\n",
    "            size=16,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            size=14,\n",
    "            color='rgb(107, 107, 107)'\n",
    "        ),\n",
    "        side='left',\n",
    "        showgrid = False\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Percentage of Total Number of Events',\n",
    "        titlefont=dict(\n",
    "            size=16,\n",
    "            color='rgb(175,238,238)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            size=14,\n",
    "            color='rgb(175,238,238)'\n",
    "        ),\n",
    "        side='right',\n",
    "        showgrid=False,\n",
    "    \n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1.0,\n",
    "        bgcolor='rgba(255, 255, 255, 0)',\n",
    "        bordercolor='rgba(255, 255, 255, 0)'\n",
    "    ),\n",
    "    barmode='group',\n",
    "    bargap=0.15,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='style-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the relationship, for each event, between the Golstein score and the number of mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_Goldstein_data = spark.read.format(\"csv\").option(\"header\", \"true\").load('./data/get_media_cov.csv')\n",
    "events_Goldstein_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_Goldstein_data = events_Goldstein_data.limit(1000).toPandas()\n",
    "events_Goldstein_data['GoldsteinScale'] = events_Goldstein_data['GoldsteinScale'].astype(float)\n",
    "events_Goldstein_data['count'] = events_Goldstein_data['count'].astype(int)\n",
    "events_Goldstein_data.rename(columns={'count': 'Mediatic Coverage'}, inplace=True)\n",
    "events_Goldstein_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,7 ))\n",
    "ax.set(yscale=\"log\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.regplot(events_Goldstein_data['GoldsteinScale'],events_Goldstein_data['Mediatic Coverage'],fit_reg =False, ax=ax, scatter_kws={\"color\": \"#1A76FF\",\"s\": 100}).set_title(\"Goldstein Index vs. Mediatic Coverage per event\")\n",
    "sns.set_style('ticks',{'legend.frameon':True,\n",
    "                                'axes.facecolor': '1'})\n",
    "sns.despine()\n",
    "ax.set_xlabel('Goldstein index',fontsize=15);\n",
    "ax.set_ylabel('Number of Mentions',fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of mentions for each QuadClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function : get_cov_quad\n",
    "df_mentions_QuadClass = spark.read.format(\"csv\").option(\"header\", \"true\").load('./data/'+'get_cov_quad_relevant.csv')\n",
    "quad_pie = get_piechart_data(df_mentions_QuadClass,'Number Mentions','QuadClass')\n",
    "quad_pie['Number Mentions'] = pd.to_numeric(quad_pie['Number Mentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_val = ['Verbal Cooperation','Material Cooperation','Verbal Conflict','Material Conflict']\n",
    "quad_pie.plot.pie(y='Number Mentions', autopct='%.2f',figsize=(5, 5),labels=get_class([int(i) for i in list(quad_pie.index)]),title = 'Quad Class type distribution in relevant mentions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of events for each quad class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_QuadClass = spark.read.format(\"csv\").option(\"header\", \"true\").load('./data/'+'get_quad.csv')\n",
    "quad_pie = get_piechart_data(df_events_QuadClass,'Number Events','QuadClass')\n",
    "quad_pie['Number Events'] = pd.to_numeric(quad_pie['Number Events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_pie.plot.pie(y='Number Events', autopct='%.2f',figsize=(5, 5),labels=get_class([int(i) for i in list(quad_pie.index)]),title = 'Quad Class type distribution in relevant events')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of events worldwide througout the months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_worldwide_time_data = pd.read_csv('./data/'+'get_events_worldwide.csv')\n",
    "events_worldwide_time_data = events_worldwide_time_data.sort_values(by='MonthYear_Date')\n",
    "events_worldwide_time_data['MonthYear_Date'] = events_worldwide_time_data['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "fig = plt.subplots(figsize=(20,5))\n",
    "plt.plot(events_worldwide_time_data['MonthYear_Date'], events_worldwide_time_data['count'],marker='o')\n",
    "plt.xlabel('Time [months]')\n",
    "plt.ylabel('Number of events worldwide')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of events worldwide during 2 years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of mentions worldwide throughout the months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_worldwide_time_data = pd.read_csv('./data/'+'get_media_coverage_worlwide.csv')\n",
    "mentions_worldwide_time_data = mentions_worldwide_time_data.sort_values(by='MentionTimeDate')\n",
    "mentions_worldwide_time_data['MentionTimeDate'] = mentions_worldwide_time_data['MentionTimeDate'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "fig = plt.subplots(figsize=(20,5))\n",
    "plt.plot(mentions_worldwide_time_data['MentionTimeDate'], mentions_worldwide_time_data['count'],marker='o')\n",
    "plt.xlabel('Time [months]')\n",
    "plt.ylabel('Number of mentions worldwide')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Number of mentions worldwide during 2 years')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Maps of Human Activity & Mediatic Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = json.load(open('countries.geo.json'))\n",
    "events_country_time_pd = pd.read_csv('./data/get_events_country_time.csv') # from function get_events_per_country\n",
    "mentions_country_time_pd = pd.read_csv('./data/get_media_cov_per_country.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missingISO(df):\n",
    "    #df = df[df.ActionGeo_CountryCode != 'US']\n",
    "    df = df[df.ActionGeo_CountryCode != 'MQ']\n",
    "    df = df[df.ActionGeo_CountryCode != 'PF']\n",
    "    df = df[df.ActionGeo_CountryCode != 'PG']\n",
    "    df = df[df.ActionGeo_CountryCode != 'CR']\n",
    "    df = df[df.ActionGeo_CountryCode != 'YI']\n",
    "    df = df[df.ActionGeo_CountryCode != 'DQ']\n",
    "    df = df[df.ActionGeo_CountryCode != 'FQ']\n",
    "    df = df[df.ActionGeo_CountryCode != 'EU']\n",
    "    df = df[df.ActionGeo_CountryCode != 'GZ']\n",
    "    df = df[df.ActionGeo_CountryCode != 'HQ']\n",
    "    df = df[df.ActionGeo_CountryCode != 'IP']\n",
    "    df = df[df.ActionGeo_CountryCode != 'JN']\n",
    "    df = df[df.ActionGeo_CountryCode != 'JQ']\n",
    "    df = df[df.ActionGeo_CountryCode != 'LQ']\n",
    "    df = df[df.ActionGeo_CountryCode != 'KQ']\n",
    "    df = df[df.ActionGeo_CountryCode != 'NT']\n",
    "    df = df[df.ActionGeo_CountryCode != 'OC']\n",
    "    df = df[df.ActionGeo_CountryCode != 'OS']\n",
    "    df = df[df.ActionGeo_CountryCode != 'RB']\n",
    "    df = df[df.ActionGeo_CountryCode != 'TE']\n",
    "    df = df[df.ActionGeo_CountryCode != 'WQ']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_country_time_pd = remove_missingISO(events_country_time_pd)\n",
    "mentions_country_time_pd = remove_missingISO(mentions_country_time_pd)\n",
    "\n",
    "events_across_times = pd.pivot_table(events_country_time_pd, values='count', index=['ActionGeo_CountryCode'], columns=['MonthYear_Date'])\n",
    "mentions_across_times = pd.pivot_table(mentions_country_time_pd, values='count', index=['ActionGeo_CountryCode'], columns=['MentionTimeDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ISO3(df):\n",
    "    iso3 = np.asarray(df.index)\n",
    "    for a, b in enumerate(df.index):\n",
    "        iso3[a]=coco.convert(names=fips2iso(b), to='ISO3',not_found=None)\n",
    "        \n",
    "    return df.set_index(iso3,'Country_Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_across_times = convert_ISO3(events_across_times)\n",
    "mentions_across_times = convert_ISO3(mentions_across_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_index1 = pd.date_range('2015-02', periods=32, freq='M')\n",
    "datetime_index2 = pd.date_range('2016-01', periods=12, freq='M')\n",
    "\n",
    "dt_index_epochs1 = datetime_index1.astype(int) // 10**9\n",
    "dt_index1 = dt_index_epochs1.astype('U10')\n",
    "\n",
    "dt_index_epochs2 = datetime_index2.astype(int) // 10**9\n",
    "dt_index2 = dt_index_epochs2.astype('U10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styledata1 = {}\n",
    "styledata2 = {}\n",
    "\n",
    "for country in events_across_times.index:\n",
    "\n",
    "    my_colors = np.empty([32])\n",
    "    \n",
    "    for s in range(32):\n",
    "        my_colors[s] = events_across_times.loc[country].values[s]\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        {'color': my_colors,\n",
    "        'opacity':1},\n",
    "        index=dt_index1\n",
    "    )\n",
    "    styledata1[country] = df\n",
    "\n",
    "\n",
    "for country in mentions_across_times.index:\n",
    "    \n",
    "    my_colors = np.empty([12])\n",
    "    \n",
    "    for s in range(12):\n",
    "        my_colors[s] = mentions_across_times.loc[country].values[s]\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        {'color': my_colors,\n",
    "        'opacity':1},\n",
    "        index=dt_index2\n",
    "    )\n",
    "    styledata2[country] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events_country_time_pd.loc[events_country_time_pd['count'].argmax()], '\\n')\n",
    "print(mentions_country_time_pd.loc[mentions_country_time_pd['count'].argmax()], '\\n')\n",
    "print(events_country_time_pd.loc[events_country_time_pd['count'].argmin()], '\\n')\n",
    "print(mentions_country_time_pd.loc[mentions_country_time_pd['count'].argmin()], '\\n')\n",
    "\n",
    "max_value1 = events_country_time_pd['count'].max()\n",
    "min_value1 = events_country_time_pd['count'].min()\n",
    "\n",
    "max_value2 = mentions_country_time_pd['count'].max()\n",
    "min_value2 = mentions_country_time_pd['count'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = (max_value1 - min_value1)/np.log(max_value1/min_value1)\n",
    "c1 = min_value1 - k1 * np.log(min_value1)\n",
    "\n",
    "k2 = (max_value2 - min_value2)/np.log(max_value2/min_value2)\n",
    "c2 = min_value2 - k2 * np.log(min_value2)\n",
    "\n",
    "events_across_times_log = events_across_times.apply(lambda x: (k1 * np.log(x)) + c1, axis = 1)\n",
    "mentions_across_times_log = mentions_across_times.apply(lambda x: (k2 * np.log(x)) + c2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap1 = linear.GnBu_04.scale(min_value1,max_value1)\n",
    "cmap2 = linear.GnBu_04.scale(min_value2,max_value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(styledata1.items()):\n",
    "    if not np.isnan(styledata1[data[0]]['color']).any():\n",
    "            styledata1[data[0]]['color'] = styledata1[data[0]]['color'].apply(cmap1)\n",
    "            \n",
    "for i, data in enumerate(styledata2.items()):\n",
    "    if not np.isnan(styledata2[data[0]]['color']).any():\n",
    "            styledata2[data[0]]['color'] = styledata2[data[0]]['color'].apply(cmap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styledict1 = {\n",
    "    str(country): data.to_dict(orient='index') for\n",
    "    country, data in styledata1.items()\n",
    "}\n",
    "\n",
    "styledict2 = {\n",
    "    str(country): data.to_dict(orient='index') for\n",
    "    country, data in styledata2.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import TimeSliderChoropleth\n",
    "\n",
    "m1 = folium.Map([0, 0], tiles='Mapbox Bright', zoom_start=1)\n",
    "\n",
    "\n",
    "g = TimeSliderChoropleth(\n",
    "    data = geo,\n",
    "    styledict=styledict1,\n",
    "    name = 'Human Activity',\n",
    "    overlay = True)\n",
    "\n",
    "m1.add_child(g)\n",
    "m1.add_child(folium.map.LayerControl())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = folium.Map([0, 0], tiles='Mapbox Bright', zoom_start=1)\n",
    "\n",
    "e = TimeSliderChoropleth(\n",
    "    data = geo,\n",
    "    styledict=styledict2,\n",
    "    name = 'Media Attention',\n",
    "    overlay = True)\n",
    "\n",
    "m2.add_child(e)\n",
    "m2.add_child(folium.map.LayerControl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.save(os.path.join('TimeSliderChoropleth_events.html'))\n",
    "m2.save(os.path.join('TimeSliderChoropleth_mentions.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static Maps of Human Activity & Mediatic Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_static = events_across_times.sum(axis = 1)\n",
    "mentions_static = mentions_across_times.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_static = pd.DataFrame(\n",
    "        {'Number Events': events_static.values},\n",
    "        index=events_static.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_static = pd.DataFrame(\n",
    "        {'Number Mentions': mentions_static.values},\n",
    "        index=mentions_static.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_countryNames(df):\n",
    "    country_names = []\n",
    "\n",
    "    for a, b in enumerate(df.index):\n",
    "        country_names.append(coco.convert(names=b, to='name_short',not_found=None))\n",
    "\n",
    "\n",
    "    df['Country Name'] = country_names\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_static = get_countryNames(events_static)\n",
    "mentions_static = get_countryNames(mentions_static)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Human activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = events_static\n",
    "\n",
    "max_static = df['Number Events'].max()\n",
    "min_static = df['Number Events'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ dict(\n",
    "        type = 'choropleth',\n",
    "        locations = df.index,\n",
    "        z = df['Number Events'],\n",
    "        text = df['Country Name'],\n",
    "        colorscale = [[0,\"rgb(247,244,249)\"],[0.35,\"rgb(204,157,204)\"],[0.5,\"rgb(223,102,177)\"],\\\n",
    "            [0.6,\"rgb(230,54,147)\"],[0.7,\"rgb(217,27,108)\"],[1,\"rgb(104,0,32)\"]],\n",
    "        autocolorscale = False,\n",
    "        reversescale = False,\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(247,244,249)',\n",
    "                width = 0.5\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            autotick = False,\n",
    "            ticksuffix = ' events',\n",
    "            title = 'Human Activity <br> (millions of events)'),\n",
    "      ) ]\n",
    "\n",
    "layout = dict(\n",
    "    title = 'Human Activity between February 2015 and September 2017<br>Source: GDELT 2.0 Dataset',\n",
    "    geo = dict(\n",
    "        showframe = False,\n",
    "        showcoastlines = False,\n",
    "        projection = dict(\n",
    "            type = 'Mercator'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "py.iplot( fig, validate=False, filename='d3-world-map' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = linear.PuRd_09.scale(min_static,max_static)\n",
    "c = (max_static - min_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame({'value': [1*c]})\n",
    "a = x['value'].apply(cmap)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame({'value': [368184]})\n",
    "b = y['value'].apply(cmap1)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mediatic coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mentions_static\n",
    "\n",
    "max_static = df['Number Mentions'].max()\n",
    "min_static = df['Number Mentions'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ dict(\n",
    "        type = 'choropleth',\n",
    "        locations = df.index,\n",
    "        z = df['Number Mentions'],\n",
    "        text = df['Country Name'],\n",
    "        colorscale = [[0,\"rgb(247,244,249)\"],[0.35,\"rgb(204,157,204)\"],[0.5,\"rgb(223,102,177)\"],\\\n",
    "            [0.6,\"rgb(230,54,147)\"],[0.7,\"rgb(217,27,108)\"],[1,\"rgb(104,0,32)\"]],\n",
    "        autocolorscale = False,\n",
    "        reversescale = False,\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(247,244,249)',\n",
    "                width = 0.5\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            autotick = False,\n",
    "            ticksuffix = ' mentions',\n",
    "            title = 'Mediatic Coverage <br> (millions of mentions)'),\n",
    "      ) ]\n",
    "\n",
    "layout = dict(\n",
    "    title = 'Mediatic Coverage between February 2015 and September 2017 <br>Source: GDELT 2.0 Dataset',\n",
    "    geo = dict(\n",
    "        showframe = False,\n",
    "        showcoastlines = False,\n",
    "        projection = dict(\n",
    "            type = 'Mercator'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "py.iplot( fig, validate=False, filename='d3-world-map' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Representation throughout times for some countries..... (human activity, media coverage, peaceful events, violent events, Goldstein score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human activity (function: for example get_events_worldwide(events_US) )\n",
    "events_US = pd.read_csv('./data/'+'events_US_time.csv')\n",
    "events_US = events_US.sort_values(by='MonthYear_Date')\n",
    "events_US['MonthYear_Date'] = events_US['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "events_AS = pd.read_csv('./data/'+'events_AS_time.csv')\n",
    "events_AS = events_AS.sort_values(by='MonthYear_Date')\n",
    "events_AS['MonthYear_Date'] = events_AS['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "events_PK = pd.read_csv('./data/'+'events_PK_time.csv')\n",
    "events_PK = events_PK.sort_values(by='MonthYear_Date')\n",
    "events_PK['MonthYear_Date'] = events_PK['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "events_SY = pd.read_csv('./data/'+'events_SY_time.csv')\n",
    "events_SY = events_SY.sort_values(by='MonthYear_Date')\n",
    "events_SY['MonthYear_Date'] = events_SY['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mentions (function : for example get_media_coverage_worldwide(mentions_US))\n",
    "mentions_US = pd.read_csv('./data/'+'mentions_US_time.csv')\n",
    "mentions_US = mentions_US.sort_values(by='MentionTimeDate')\n",
    "mentions_US['MentionTimeDate'] = mentions_US['MentionTimeDate'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "mentions_AS = pd.read_csv('./data/'+'mentions_AS_time.csv')\n",
    "mentions_AS = mentions_AS.sort_values(by='MentionTimeDate')\n",
    "mentions_AS['MentionTimeDate'] = mentions_AS['MentionTimeDate'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "mentions_PK = pd.read_csv('./data/'+'mentions_PK_time.csv')\n",
    "mentions_PK = mentions_PK.sort_values(by='MentionTimeDate')\n",
    "mentions_PK['MentionTimeDate'] = mentions_PK['MentionTimeDate'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "mentions_SY = pd.read_csv('./data/'+'mentions_SY_time.csv')\n",
    "mentions_SY = mentions_SY.sort_values(by='MentionTimeDate')\n",
    "mentions_SY['MentionTimeDate'] = mentions_SY['MentionTimeDate'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Goldstein score (function : for example get_Goldstein(events_US))\n",
    "goldstein_US = pd.read_csv('./data/'+'Goldstein_US.csv')\n",
    "goldstein_US = goldstein_US.sort_values(by='MonthYear_Date')\n",
    "goldstein_US['MonthYear_Date'] = goldstein_US['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "goldstein_AS = pd.read_csv('./data/'+'Goldstein_AS.csv')\n",
    "goldstein_AS = goldstein_AS.sort_values(by='MonthYear_Date')\n",
    "goldstein_AS['MonthYear_Date'] = goldstein_AS['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "goldstein_PK = pd.read_csv('./data/'+'Goldstein_PK.csv')\n",
    "goldstein_PK = goldstein_PK.sort_values(by='MonthYear_Date')\n",
    "goldstein_PK['MonthYear_Date'] = goldstein_PK['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "goldstein_SY = pd.read_csv('./data/'+'Goldstein_SY.csv')\n",
    "goldstein_SY = goldstein_SY.sort_values(by='MonthYear_Date')\n",
    "goldstein_SY['MonthYear_Date'] = goldstein_SY['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violent events and peaceful events : human activity\n",
    "activity_by_type = pd.read_csv('./data/'+'get_activity_byTypeCountry_time.csv')\n",
    "\n",
    "peaceful_events = activity_by_type[activity_by_type['EventRootCode'].isin(['01','02','03'])]\n",
    "violent_events = activity_by_type[activity_by_type['EventRootCode'].isin(['18','19','20'])]\n",
    "\n",
    "peaceful_events_US = peaceful_events[peaceful_events['ActionGeo_CountryCode']=='US'].sort_values(by='MonthYear_Date')\n",
    "peaceful_events_US = peaceful_events_US.groupby(by='MonthYear_Date',as_index=False).sum().sort_values(by='MonthYear_Date').drop(columns='EventRootCode')\n",
    "peaceful_events_US['MonthYear_Date'] = peaceful_events_US['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "peaceful_events_AS = peaceful_events[peaceful_events['ActionGeo_CountryCode']=='AS'].sort_values(by='MonthYear_Date')\n",
    "peaceful_events_AS = peaceful_events_AS.groupby(by='MonthYear_Date',as_index=False).sum().sort_values(by='MonthYear_Date').drop(columns='EventRootCode')\n",
    "peaceful_events_AS['MonthYear_Date'] = peaceful_events_AS['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "\n",
    "peaceful_events_PK = peaceful_events[peaceful_events['ActionGeo_CountryCode']=='PK'].sort_values(by='MonthYear_Date')\n",
    "peaceful_events_PK = peaceful_events_PK.groupby(by='MonthYear_Date',as_index=False).sum().sort_values(by='MonthYear_Date').drop(columns='EventRootCode')\n",
    "peaceful_events_PK['MonthYear_Date'] = peaceful_events_PK['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "\n",
    "peaceful_events_SY = peaceful_events[peaceful_events['ActionGeo_CountryCode']=='SY'].sort_values(by='MonthYear_Date')\n",
    "peaceful_events_SY = peaceful_events_SY.groupby(by='MonthYear_Date',as_index=False).sum().sort_values(by='MonthYear_Date').drop(columns='EventRootCode')\n",
    "peaceful_events_SY['MonthYear_Date'] = peaceful_events_SY['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "\n",
    "violent_events_US = violent_events[violent_events['ActionGeo_CountryCode']=='US'].sort_values(by='MonthYear_Date')\n",
    "violent_events_US = violent_events_US.groupby(by='MonthYear_Date',as_index=False).sum().sort_values(by='MonthYear_Date').drop(columns='EventRootCode')\n",
    "violent_events_US['MonthYear_Date'] = violent_events_US['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "violent_events_AS = violent_events[violent_events['ActionGeo_CountryCode']=='AS'].sort_values(by='MonthYear_Date')\n",
    "violent_events_AS = violent_events_AS.groupby(by='MonthYear_Date',as_index=False).sum().sort_values(by='MonthYear_Date').drop(columns='EventRootCode')\n",
    "violent_events_AS['MonthYear_Date'] = violent_events_AS['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "\n",
    "violent_events_SY = violent_events[violent_events['ActionGeo_CountryCode']=='SY'].sort_values(by='MonthYear_Date')\n",
    "violent_events_SY = violent_events_SY.groupby(by='MonthYear_Date',as_index=False).sum().sort_values(by='MonthYear_Date').drop(columns='EventRootCode')\n",
    "violent_events_SY['MonthYear_Date'] = violent_events_SY['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "\n",
    "violent_events_PK = violent_events[violent_events['ActionGeo_CountryCode']=='PK'].sort_values(by='MonthYear_Date')\n",
    "violent_events_PK = violent_events_PK.groupby(by='MonthYear_Date',as_index=False).sum().sort_values(by='MonthYear_Date').drop(columns='EventRootCode')\n",
    "violent_events_PK['MonthYear_Date'] = violent_events_PK['MonthYear_Date'].astype(str).apply(lambda x : datetime.strptime(x,'%Y%m').strftime('%m-%Y'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### USA #####\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='ADAforever', api_key='4uSLbBobfv0jcAMcetam')\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=events_US['MonthYear_Date'],\n",
    "    y=events_US['count'],\n",
    "    name='Events',\n",
    "    yaxis='y1',\n",
    "    line = dict(\n",
    "        color = '#1f77b4')\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=goldstein_US['MonthYear_Date'],\n",
    "    y=goldstein_US['avg(GoldsteinScale)'],\n",
    "    name='Goldstein score',\n",
    "    yaxis='y2',\n",
    "    line = dict(\n",
    "        color = '#ff7f0e')\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=peaceful_events_US['MonthYear_Date'],\n",
    "    y=peaceful_events_US['count'],\n",
    "    name='Peaceful events',\n",
    "    yaxis='y3',\n",
    "    line = dict(\n",
    "        color = '#d62728')\n",
    "    \n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x=violent_events_US['MonthYear_Date'],\n",
    "    y=violent_events_US['count'],\n",
    "    name='Violent events',\n",
    "    yaxis='y3',\n",
    "    line = dict(\n",
    "        color = '#d62728',\n",
    "        dash = 'dot')\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x=mentions_US['MentionTimeDate'],\n",
    "    y=mentions_US['count'],\n",
    "    name='Mentions',\n",
    "    yaxis='y4',\n",
    "    line = dict(\n",
    "        color = '#9467bd')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4, trace5]\n",
    "layout = go.Layout(\n",
    "    title='Evolution across the months for the United States of America',\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        domain=[0.02, 0.7],\n",
    "        showgrid=False),\n",
    "    yaxis=dict(\n",
    "        title='Number of events',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#1f77b4'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#1f77b4'\n",
    "        ),\n",
    "        side='right',\n",
    "        position=0.72\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Average Goldstein score',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#ff7f0e'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#ff7f0e'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='left',\n",
    "        position=0\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        title='Number of events (pacific or violent)',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#d62728'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#d62728'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        position=0.78\n",
    "    ),\n",
    "    yaxis4=dict(\n",
    "        title='Number of mentions',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#9467bd'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#9467bd'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        position=0.84\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-multiple')\n",
    "py.iplot(fig,filename = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AUSTRALIA #####\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='ADAforever', api_key='4uSLbBobfv0jcAMcetam')\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=events_AS['MonthYear_Date'],\n",
    "    y=events_AS['count'],\n",
    "    name='Events',\n",
    "    yaxis='y1',\n",
    "    line = dict(\n",
    "        color = '#1f77b4')\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=goldstein_AS['MonthYear_Date'],\n",
    "    y=goldstein_AS['avg(GoldsteinScale)'],\n",
    "    name='Goldstein score',\n",
    "    yaxis='y2',\n",
    "    line = dict(\n",
    "        color = '#ff7f0e')\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=peaceful_events_AS['MonthYear_Date'],\n",
    "    y=peaceful_events_AS['count'],\n",
    "    name='Peaceful events',\n",
    "    yaxis='y3',\n",
    "    line = dict(\n",
    "        color = '#d62728')\n",
    "    \n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x=violent_events_AS['MonthYear_Date'],\n",
    "    y=violent_events_AS['count'],\n",
    "    name='Violent events',\n",
    "    yaxis='y3',\n",
    "    line = dict(\n",
    "        color = '#d62728',\n",
    "        dash = 'dot')\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x=mentions_AS['MentionTimeDate'],\n",
    "    y=mentions_AS['count'],\n",
    "    name='Mentions',\n",
    "    yaxis='y4',\n",
    "    line = dict(\n",
    "        color = '#9467bd')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4, trace5]\n",
    "layout = go.Layout(\n",
    "    title='Evolution across the months for Australia',\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        domain=[0.02, 0.7],\n",
    "        showgrid=False),\n",
    "    yaxis=dict(\n",
    "        title='Number of events',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#1f77b4'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#1f77b4'\n",
    "        ),\n",
    "        side='right',\n",
    "        position=0.72\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Average Goldstein score',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#ff7f0e'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#ff7f0e'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='left',\n",
    "        position=0\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        title='Number of events (pacific or violent)',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#d62728'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#d62728'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        position=0.78\n",
    "    ),\n",
    "    yaxis4=dict(\n",
    "        title='Number of mentions',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#9467bd'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#9467bd'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        position=0.84\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-multiple')\n",
    "py.iplot(fig,filename = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PAKISTAN #####\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='ADAforever', api_key='4uSLbBobfv0jcAMcetam')\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=events_PK['MonthYear_Date'],\n",
    "    y=events_PK['count'],\n",
    "    name='Events',\n",
    "    yaxis='y1',\n",
    "    line = dict(\n",
    "        color = '#1f77b4')\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=goldstein_PK['MonthYear_Date'],\n",
    "    y=goldstein_PK['avg(GoldsteinScale)'],\n",
    "    name='Goldstein score',\n",
    "    yaxis='y2',\n",
    "    line = dict(\n",
    "        color = '#ff7f0e')\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=peaceful_events_PK['MonthYear_Date'],\n",
    "    y=peaceful_events_PK['count'],\n",
    "    name='Peaceful events',\n",
    "    yaxis='y3',\n",
    "    line = dict(\n",
    "        color = '#d62728')\n",
    "    \n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x=violent_events_PK['MonthYear_Date'],\n",
    "    y=violent_events_PK['count'],\n",
    "    name='Violent events',\n",
    "    yaxis='y3',\n",
    "    line = dict(\n",
    "        color = '#d62728',\n",
    "        dash = 'dot')\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x=mentions_PK['MentionTimeDate'],\n",
    "    y=mentions_PK['count'],\n",
    "    name='Mentions',\n",
    "    yaxis='y4',\n",
    "    line = dict(\n",
    "        color = '#9467bd')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4, trace5]\n",
    "layout = go.Layout(\n",
    "    title='Evolution across the months for Pakistan',\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        domain=[0.02, 0.7],\n",
    "        showgrid=False),\n",
    "    yaxis=dict(\n",
    "        title='Number of events',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#1f77b4'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#1f77b4'\n",
    "        ),\n",
    "        side='right',\n",
    "        position=0.72\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Average Goldstein score',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#ff7f0e'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#ff7f0e'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='left',\n",
    "        position=0\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        title='Number of events (pacific or violent)',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#d62728'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#d62728'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        position=0.78\n",
    "    ),\n",
    "    yaxis4=dict(\n",
    "        title='Number of mentions',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#9467bd'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#9467bd'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        position=0.84\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-multiple')\n",
    "py.iplot(fig,filename = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SYRIA #####\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='ADAforever', api_key='4uSLbBobfv0jcAMcetam')\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=events_SY['MonthYear_Date'],\n",
    "    y=events_SY['count'],\n",
    "    name='Events',\n",
    "    yaxis='y1',\n",
    "    line = dict(\n",
    "        color = '#1f77b4')\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=goldstein_SY['MonthYear_Date'],\n",
    "    y=goldstein_SY['avg(GoldsteinScale)'],\n",
    "    name='Goldstein score',\n",
    "    yaxis='y2',\n",
    "    line = dict(\n",
    "        color = '#ff7f0e')\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=peaceful_events_SY['MonthYear_Date'],\n",
    "    y=peaceful_events_SY['count'],\n",
    "    name='Peaceful events',\n",
    "    yaxis='y3',\n",
    "    line = dict(\n",
    "        color = '#d62728')\n",
    "    \n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x=violent_events_SY['MonthYear_Date'],\n",
    "    y=violent_events_SY['count'],\n",
    "    name='Violent events',\n",
    "    yaxis='y3',\n",
    "    line = dict(\n",
    "        color = '#d62728',\n",
    "        dash = 'dot')\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x=mentions_SY['MentionTimeDate'],\n",
    "    y=mentions_SY['count'],\n",
    "    name='Mentions',\n",
    "    yaxis='y4',\n",
    "    line = dict(\n",
    "        color = '#9467bd')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4, trace5]\n",
    "layout = go.Layout(\n",
    "    title='Evolution across the months for Syria',\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        domain=[0.02, 0.7],\n",
    "        showgrid=False),\n",
    "    yaxis=dict(\n",
    "        title='Number of events',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#1f77b4'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#1f77b4'\n",
    "        ),\n",
    "        side='right',\n",
    "        position=0.72\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Average Goldstein score',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#ff7f0e'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#ff7f0e'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='left',\n",
    "        position=0\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        title='Number of events (pacific or violent)',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#d62728'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#d62728'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        position=0.78\n",
    "    ),\n",
    "    yaxis4=dict(\n",
    "        title='Number of mentions',\n",
    "        showgrid=False,\n",
    "        titlefont=dict(\n",
    "            color='#9467bd'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='#9467bd'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        position=0.84\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-multiple')\n",
    "py.iplot(fig,filename = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation for some countries over the 2 years of the number of peaceful and violent events, as well as the average media coverage per event for these events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of peaceful and violent events\n",
    "peaceUS = peaceful_events_US['count'].sum()\n",
    "peaceAS = peaceful_events_AS['count'].sum()\n",
    "peacePK = peaceful_events_PK['count'].sum()\n",
    "peaceSY = peaceful_events_SY['count'].sum()\n",
    "\n",
    "violentUS = violent_events_US['count'].sum()\n",
    "violentAS = violent_events_AS['count'].sum()\n",
    "violentPK = violent_events_PK['count'].sum()\n",
    "violentSY = violent_events_SY['count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average media coverage per violent and per peaceful events\n",
    "average_media_coverage_by_type = pd.read_csv('./data/'+'get_av_media_coverage_country_type.csv')\n",
    "\n",
    "peaceful_coverage = average_media_coverage_by_type[average_media_coverage_by_type['EventRootCode'].isin(['01','02','03'])]\n",
    "violent_coverage = average_media_coverage_by_type[average_media_coverage_by_type['EventRootCode'].isin(['18','19','20'])]\n",
    "\n",
    "peace_coverage_US = peaceful_coverage[peaceful_coverage['ActionGeo_CountryCode']=='US']['Average media coverage per event'].mean()\n",
    "peace_coverage_AS = peaceful_coverage[peaceful_coverage['ActionGeo_CountryCode']=='AS']['Average media coverage per event'].mean()\n",
    "peace_coverage_PK = peaceful_coverage[peaceful_coverage['ActionGeo_CountryCode']=='PK']['Average media coverage per event'].mean()\n",
    "peace_coverage_SY = peaceful_coverage[peaceful_coverage['ActionGeo_CountryCode']=='SY']['Average media coverage per event'].mean()\n",
    "\n",
    "violent_coverage_US = violent_coverage[violent_coverage['ActionGeo_CountryCode']=='US']['Average media coverage per event'].mean()\n",
    "violent_coverage_AS = violent_coverage[violent_coverage['ActionGeo_CountryCode']=='AS']['Average media coverage per event'].mean()\n",
    "violent_coverage_PK = violent_coverage[violent_coverage['ActionGeo_CountryCode']=='PK']['Average media coverage per event'].mean()\n",
    "violent_coverage_SY = violent_coverage[violent_coverage['ActionGeo_CountryCode']=='SY']['Average media coverage per event'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pacific and violent events\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "from plotly import tools\n",
    "plotly.tools.set_credentials_file(username='ADAforever', api_key='4uSLbBobfv0jcAMcetam')\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=['USA', 'Australia', 'Pakistan', 'Syria'],\n",
    "    y=[peaceUS,peaceAS,peacePK,peaceSY],\n",
    "    name='Peaceful events',\n",
    "    marker=dict(\n",
    "        color='rgb(26, 118, 255)'\n",
    "    ),\n",
    "    width = 0.6\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=['USA', 'Australia', 'Pakistan', 'Syria'],\n",
    "    y=[violentUS,violentAS,violentPK,violentSY],\n",
    "    name='Violent events',\n",
    "    marker=dict(\n",
    "        color='rgb(175,238,238)'\n",
    "    ),\n",
    "    width=0.6\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    width=500,\n",
    "    barmode='stack',\n",
    "    title='Number of events'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of mentions per event for violent and peaceful events\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='ADAforever', api_key='4uSLbBobfv0jcAMcetam')\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=['USA', 'Australia', 'Pakistan', 'Syria'],\n",
    "    y=[peace_coverage_US,peace_coverage_AS,peace_coverage_PK,peace_coverage_SY],\n",
    "    name='Peaceful events',\n",
    "    marker=dict(\n",
    "        color='rgb(26, 118, 255)'\n",
    "    ),\n",
    "    width = 0.6\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=['USA', 'Australia', 'Pakistan', 'Syria'],\n",
    "    y=[violent_coverage_US,violent_coverage_AS,violent_coverage_PK,violent_coverage_SY],\n",
    "    name='Violent events',\n",
    "    marker=dict(\n",
    "        color='rgb(175,238,238)'\n",
    "    ),\n",
    "    width = 0.6\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    width=500,\n",
    "    title='Average media coverage per event',\n",
    "    barmode='stack'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEFT TO DO : Goldstein and media coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPRESENTATION FINALE\n",
    "avg_media_Golstein = pd.read_csv('./data/'+'Goldstein_mediaCov_average.csv')\n",
    "avg_media_Golstein = avg_media_Golstein.orderBy(by='GoldsteinScale')\n",
    "avg_media_Golstein_data.plot.bar(x='GoldsteinScale', y='Average media coverage per event', figsize=(8, 8), colormap='Paired', \n",
    "                    title='Distribution of average media coverage per event by Goldstein score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPRESENTATION FINALE\n",
    "goldstein_mediaCov = pd.read_csv('./data/'+'Goldstein_mediaCov.csv')\n",
    "print(stats.spearmanr(goldstein_mediaCov['GoldsteinScale'],goldstein_mediaCov['Number Mentions']))\n",
    "print(stats.pearsonr(normalize(goldstein_mediaCov['GoldsteinScale']),normalize(goldstein_mediaCov['Number Mentions'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that normalize a data serie, by shifting its mean to 0 and variance to 1\n",
    "def normalize(serie):\n",
    "    return serie.apply(lambda x : (x-serie.mean())/serie.std())\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Pearson r considers normal distributions - normalize data before computing the pearson coefficient\n",
    "stats.pearsonr(normalize(number_events),normalize(number_mentions))\n",
    "# Speaman coefficient does not consider normal distribution , computes monotonic correlation\n",
    "stats.spearmanr(number_events,number_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation using just one value per Goldstein score\n",
    "mentions_Golstein_data['GoldsteinScale']=pd.to_numeric(mentions_Golstein_data['GoldsteinScale'])\n",
    "mentions_Golstein_data['Number Mentions']=pd.to_numeric(mentions_Golstein_data['Number Mentions'])\n",
    "stats.spearmanr(mentions_Golstein_data['GoldsteinScale'],mentions_Golstein_data['Number Mentions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
